# video-analytics-paper

## 📚 目录

- [AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics](#accdecoder-accelerated-decoding-for-neural-enhanced-video-analytics)
- [Accelerated Neural Enhancement for Video Analytics With Video Quality Adaptation](#accelerated-neural-enhancement-for-video-analytics-with-video-quality-adaptation)
- [Spatialyze: A Geospatial Video Analytics System with Spatial-Aware Optimizations](#spatialyze-a-geospatial-video-analytics-system-with-spatial-aware-optimizations)
- [Boosting Neural Representations for Videos with a Conditional Decoder](#boosting-neural-representations-for-videos-with-a-conditional-decoder)
- [Region-based Content Enhancement for Efficient Video Analytics at the Edge](#region-based-content-enhancement-for-efficient-video-analytics-at-the-edge)
- [Accelerating Aggregation Queries on Unstructured Streams of Data](#accelerating-aggregation-queries-on-unstructured-streams-of-data)
- [COVA: Exploiting Compressed-Domain Analysis to Accelerate Video Analytics](#cova-exploiting-compressed-domain-analysis-to-accelerate-video-analytics)
- [Gemel: Model Merging for Memory-Efficient, Real-Time Video Analytics at the Edge](#gemel-model-merging-for-memory-Efficient-real-time-video-analytics-at-the-edge)
- [Extract-Transform-Load for Video Streams](#extract-transform-load-for-video-streams)
- [GRACE: Loss-Resilient Real-Time Video through Neural Codecs](#grace-loss-resilient-real-time-video-through-neural-codecs)
- [Edge-assisted Adaptive Configuration for Serverless-based Video Analytics](#edge-assisted-adaptive-configuration-for-serverless-based-video-analytics)
- [PacketGame: Multi-Stream Packet Gating for Concurrent Video Inference at Scale](#packetgame-multi-stream-packet-gating-for-concurrent-video-inference-at-scale)
- [InFi: End-to-end Learnable Input Filter for Resource-efficient Mobile-centric Inference](#InFi-end-to-end-learnable-input-filter-for-resource-efficient-mobile-centric-inference)
- [MadEye: Boosting Live Video Analytics Accuracy with Adaptive Camera Configurations](#MadEye-boosting-live-video-analytics-accuracy-with-adaptive-camera-configurations)
- [NeuroScaler: Neural Video Enhancement at Scale](#NeuroScaler-neural-video-enhancement-at-scale)
- [NEMO: Enabling Neural-enhanced Video Streaming on Commodity Mobile Devices](#NEMO-enabling-neural-enhanced-video-streaming-on-commodity-mobile-devices)
- [Optimizing Video Selection LIMIT Queries With Commonsense Knowledge](#Optimizing-video-selection-limit-queries-with-commonsense-knowledge)
- [Reducto: On-Camera Filtering for Resource-Efficient Real-Time Video Analytics](#Reducto-On-Camera-filtering-for-resource-efficient-real-time-video-analytics)
- [TASM: A Tile-Based Storage Manager for Video Analytics](#TASM-A-Tile-based-storage-manager-for-video-analytics)
- [SEIDEN: Revisiting Query Processing in Video Database Systems](#SEIDEN-revisiting-query-processing-in-video-database-systems)
- [TVM: A Tile-based Video Management Framework](#TVM-A-Tile-based-video-management-framework)
- [TileClipper: Lightweight Selection of Regions of Interest from Videos for Traffic Surveillance](#TileClipper-Lightweight-selection-of-regions-of-interest-from-videos-for-traffic-surveillance)





## AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics

**本文由哥廷根大学完成，发表在 INFOCOM 2023。代码见：https://github.com/mi150/AccDecoder**

### 1. 需要解决的问题

低质量的视频导致视频分析准确率下降，需要对视频内容进行增强从而提高准确率。常规的方法要么花费时间过多，要么无法很好地适应视频内容的动态性，导致准确率下降。因此本文需要解决的问题是：

> 如何在保证视频分析准确率的同时，尽量降低时延，达到一个良好的权衡？

### 2. 解决方案——AccDecoder

AccDecoder 将视频分析流程分为三个步骤：

- **超分辨率增强（SR）**：为了降低延迟同时保证分析准确率，本文选择少量视频帧（称为锚帧）进行超分辨率增强。其他低分辨率帧结合锚帧、运动向量和残差合成高分辨率帧。

- **DNN 推理**：为了进一步降低延迟，仅对推理帧运行 DNN，对非推理帧则重用推理结果，并结合运动向量进行调整。

- **重用结果**：结合视频连续性和先前结果，提高效率并减少冗余计算。

### 3. 如何选择用于 SR 和推理的帧？——使用马尔可夫决策过程（MDP）

- **状态（State）**：智能体决策的依据。包括关键帧的内容特征、视频块内部的帧间差异。

- **动作（Action）**：智能体根据当前状态做出的决策。每个视频块的帧根据两个阈值分类：

  - `tr1`（SR Threshold）：用于选择锚点帧（做超分增强）  
  - `tr2`（Inference Threshold）：用于选择推理帧（执行 DNN）

- **奖励（Reward）**：智能体根据在给定时延约束下的分析准确率，来衡量策略好坏，目标是最大化准确率。


## Accelerated Neural Enhancement for Video Analytics With Video Quality Adaptation
**本文由哥廷根大学和南京大学共同完成，发表在 TON 2024 ,是 AccDecoder（INFOCOM 2023） 的扩展。**

### 1.动机： 处理来自不同摄像头或自适应编码器（如 AWStream, Pensieve, Chameleon）的异构分辨率视频流。

- **核心改进**： 在 DRL 调度器的状态 (State) 中显式加入视频块的分辨率信息。

### 2.效果： 调度器能学习针对不同分辨率块的最优阈值设置。

- **对低分辨率块**：倾向于选择更多帧做 SR（成本相对低，精度提升潜力大）。

- **对高分辨率块**：更谨慎选择 SR 帧（成本高）。

### 3.优势： 显著提升了 AccDecoder 的通用性 (Generality) 和可扩展性 (Scalability)，能更好适应实际中多源、自适应码率的视频流。


## Spatialyze: A Geospatial Video Analytics System with Spatial-Aware Optimizations  
**本文由 UC Berkeley 完成，发表在 VLDB 2024，项目主页见 https://spatialyze.github.io/**

### 1. 动机：传统视频分析系统未充分利用摄像头元数据（如地理位置、时间、姿态等）

- **问题核心**：视频实际采集自真实世界的物理空间，但现有方法大多只处理图像帧本身，忽略了空间语义与物理约束。

### 2. 方法：Spatialyze 系统通过空间元数据优化视频查询流程，共包含三个阶段：

- **构建阶段**：用户定义虚拟世界（摄像头布局、地理信息等）  
- **过滤阶段**：用户以声明式语言描述感兴趣的场景（如“在十字路口出现的人”）  
- **观察阶段**：系统执行完整分析流程，包括数据整合、视频处理、对象查询和结果输出

其中，**视频处理器模块**采用了四项空间感知优化技术：

- **Road Visibility Pruner**：跳过未覆盖目标区域的相机帧  
- **Object Type Pruner**：仅处理用户关心的目标类型  
- **Geometry-Based 3D Location Estimator**：根据边界框推算目标的 3D 物理位置  
- **Exit Frame Sampler**：基于事件预测减少不必要的帧处理

### 3. 效果：相比未优化系统，Spatialyze 在保持 97.1% 准确率的同时，提升了 5.3 倍的执行效率


## Boosting Neural Representations for Videos with a Conditional Decoder
**本文由香港科技大学、商汤科技、香港中文大学等机构合作完成，发表在 CVPR 2023。项目主页见 https://github.com/Xinjie-Q/Boosting-NeRV**

### 1. 动机：现有隐式视频表示（INRs）未能充分发挥其潜力

- **问题核心**：在解码目标视频帧时，网络内部的**中间特征与目标帧对齐不充分**，且压缩流程存在**不一致性**，导致重建质量和压缩效率受限。

### 2. 方法：提出一个通用的增强框架，通过引入条件解码器和一致性压缩来提升性能

该框架主要包含四项核心技术创新：

- **时间感知的条件解码器 (Temporal-aware Conditional Decoder)**：利用**帧索引**作为条件，通过一个无归一化的**仿射变换模块 (TAT)** 动态调整中间特征，实现特征与目标帧的精确对齐。

- **正弦NeRV类块 (Sinusoidal NeRV-like Block)**：采用 **SINE (正弦)** 作为激活函数，以生成更多样化的特征，并以更少的参数提升模型容量。

- **高频信息保留的损失函数 (High-frequency-preserving Loss)**：组合**频域L1损失**、空域L1损失和MS-SSIM损失，以更好地保留视频的边缘和纹理细节。

- **一致的熵最小化压缩 (Consistent Entropy Minimization, CEM)**：采用简单的**高斯模型**替代复杂的代理网络来估计熵，保证了训练和推理阶段的**完全一致性**，从而优化了率失真性能。

### 3. 效果：在多种视频任务上显著优于基线，并达到SOTA压缩性能

- **视频回归**：在UVG数据集上，相比基线模型平均提升 **1.2-1.7 dB PSNR**，且收敛速度更快。
- **视频压缩**：RD性能显著优于传统编解码器（H.264/H.265）和部分SOTA学习方法（DCVC），展示了极强的竞争力。
- **视频修复与插值**：在修复任务上平均带来 **1.9-4.2 dB** 的性能提升，并在插值任务上表现更优。


## Region-based Content Enhancement for Efficient Video Analytics at the Edge 
**本文由清华大学、南京大学、哥廷根大学等机构合作完成，发表在  NSDI '25。项目主页见 https://github.com/mi150/RegenHance**

### 1. 动机：现有内容增强方法在边缘视频分析中存在性能瓶颈

- **问题核心**：现有内容增强方法（如超分辨率）在应用于边缘视频分析时存在**性能与精度的根本矛盾**。对**整个视频帧进行增强**会消耗海量计算资源，导致吞吐量极低，无法实时处理；而**选择性地增强部分关键帧**又会因信息复用造成精度大幅下降，无法满足AI分析任务的苛刻要求。

### 2. 方法：提出一个名为 RegenHance 的系统，通过只增强关键区域来打破性能与精度的困境

该系统通过三大创新组件，实现了高效率与高精度的平衡：

- **基于宏块的区域重要性预测 (Macroblock-based Region Importance Prediction)**：采用视频编码中的**宏块 (`Macroblock`)** 作为基本单元，通过一个超轻量级预测模型，在原始低质量视频上**快速且准确地定位**出对分析精度提升最大的关键区域。

- **区域感知的打包增强 (Region-aware Enhancement)**：将识别出的零散、不规则的关键区域高效地**“拼接打包”**成一个或多个紧凑的矩形张量。该过程被建模为一个二维装箱问题，只对打包后的小尺寸张量进行增强，极大降低了计算开销。

- **基于配置文件的执行规划 (Profile-based Execution Planning)**：通过**离线剖析**各组件（解码、预测、增强、分析）在不同负载下的性能，在线为整个分析流水线动态生成最优执行计划，确保CPU/GPU资源被充分利用，最大化端到端吞吐量。

### 3. 效果：在精度和吞吐量上全面超越SOTA方法，实现显著性能提升

- **精度与吞吐量双重提升**：在目标检测和语义分割任务上，相比SOTA的帧级增强方法（如Nemo、NeuroScaler），**分析精度提升10-19%**，同时**端到端吞吐量提升2-3倍**。
- **资源利用效率极高**：相比全帧增强，可节省**高达77%的GPU计算资源**，将宝贵的算力集中用于对分析任务真正有益的区域。
- **强大的平台通用性**：在五种异构硬件平台（从云端A100到边缘Jetson）上均表现出**强大的鲁棒性和有效性**，证明了其在真实边缘计算场景中的实用价值。


## Accelerating Aggregation Queries on Unstructured Streams of Data
**本文由斯坦福大学、芝加哥大学、伊利诺伊大学厄巴纳-香槟分校合作完成，发表在会议 VLDB '23。项目主页见 https://github.com/stanford-futuredata/InQuest**

### 1. 动机：现有数据分析系统在处理非结构化数据流时面临根本性困境

- **问题核心**：现有数据分析系统在处理视频、文本等非结构化数据流的聚合查询时，存在**实时性、成本与准确性**的根本矛盾。对**数据流中的每一个元素**（如每一帧视频、每一条推文）都运行高精度的深度学习模型（**Oracle**）会产生巨大的计算开销和延迟，无法满足实时性要求；而简单的采样或使用低精度模型又无法保证查询结果的**准确性**。

- **现有方案的瓶颈**：
    - **批处理系统（Batch Systems）**：许多先进的查询加速系统（如 ABae）被设计为批处理模式，它们需要**预先访问整个数据集**来规划采样策略或训练专用模型，这在数据持续产生、无法预知未来的流式场景（Streaming Scenarios）中完全不适用。
    - **领域特定系统（Domain-Specific Systems）**：部分流式处理系统仅为特定数据模态（如仅视频）或特定模型架构设计，缺乏在多模态数据上进行通用查询的能力。

### 2. 方法：提出一个名为 InQuest 的系统，通过智能自适应的采样框架来解决上述困境

InQuest 将数据流切分为 **数据段**（Segments）进行处理，其核心是一个由三大创新组件构成的闭环自适应学习系统，旨在以最低的 **Oracle 调用成本** 实现最高的查询精度。

- **廉价代理引导的动态分层 (Proxy-Guided Dynamic Stratification)**：系统不直接处理原始数据，而是首先使用一个**超轻量级的代理模型（Proxy）**为数据流中的每个元素快速生成一个分数。InQuest 以此分数为基础，将数据流动态地划分为多个内部同质的**分层（Strata）**。该分层策略并非固定不变，而是通过分析上一个数据段的 Proxy 分数**分位数（Quantiles）**，并结合**指数加权移动平均（EWMA）**进行平滑，从而能够持续地**自适应**于数据分布的实时变化。

- **基于统计的自适应预算分配 (Statistics-driven Adaptive Budget Allocation)**：在分层的基础上，InQuest 智能地决定在每个分层中投入多少 **Oracle 调用预算**。该决策依据经典的**奈曼分配（Neyman Allocation）**理论，会优先将更多预算分配给：1) **不确定性更高**（即估计的**标准差 `σ̂`** 更大）的分层；2) **信息价值更高**（即估计的**谓词为正比例 `p̂`** 更高）的分层。同时，系统引入**防御性采样（Defensive Sampling）**机制，为每个分层预留一小部分固定预算，极大地增强了算法在面对突变时的**鲁棒性**。整个分配方案同样通过EWMA进行平滑，以实现稳健的持续学习。

- **保证流式无偏的采样执行 (Unbiased Streaming Sample Execution)**：在确定了各分层的采样数量后，由于无法预知每个分层在当前数据段的总大小，InQuest 为每个分层部署了一个独立的**水塘抽样（Reservoir Sampling）**。该技术能够在不知道数据流总长度的情况下，公平、无偏地从该分层的所有元素中抽取指定数量的样本。只有被水塘抽样选中的样本，才会“按需”触发昂贵的 Oracle 调用，从而将计算成本降至最低，并保证了最终统计估计的**无偏性**。

### 3. 效果：在查询精度、系统吞吐量和成本节约上均取得SOTA性能

- **成本与吞吐量巨幅优化**：在达到与流式基线方法（均匀采样、固定分层采样）相同的查询精度（RMSE）时，InQuest 所需的 **Oracle 调用次数（即成本）减少了高达 5.0 倍**，显著提升了系统的处理吞吐量。

- **查询精度显著提升**：在固定的 Oracle 调用预算下，相比为批处理场景设计的 SOTA 方法（如 ABae），InQuest 在流式场景下的**查询均方根误差（RMSE）降低了高达 1.9 倍**。这证明了其分段学习策略能有效利用数据的时序局部性，甚至超越了“总览全局”的批处理方法。

- **强大的鲁棒性与通用性**：
    - 通过**对抗性漂移测试**，证明了 InQuest 即使在数据分布频繁突变的非平稳流上依然能保持稳健性能，展现了其强大的自适应能力。
    - 在**视频和文本两种模态**的六个真实世界数据集上均验证了其有效性，证明了其框架的**通用性**。
    - **消融实验**证明了其动态分层和动态预算分配两大核心组件对于实现高性能均不可或缺。


## COVA: Exploiting Compressed-Domain Analysis to Accelerate Video Analytics
**本文由韩国科学技术院（KAIST）与谷歌（Google）合作完成，发表在会议 USENIX ATC '22。项目主页见 https://github.com/casys-kaist/CoVA**

### 1. 动机：现有视频分析系统在处理大规模视频数据时面临新的性能瓶颈

- **问题核心**：现代视频分析系统通过级联（Cascade）架构，成功缓解了昂贵的深度学习模型（DNN）的推理瓶颈。然而，它们普遍忽视或规避了一个更基础的瓶颈：**视频解码**。在对任何一帧进行分析前，必须先将压缩的视频流（如H.264格式）解码为原始像素数据，这一过程本身就非常缓慢，尤其是在处理高清视频时，解码速度甚至远低于后续的分析速度，成为了整个系统的**新瓶颈**。

- **现有方案的瓶颈**：
    - **高昂的预处理成本**：现有系统为了绕开解码瓶颈，通常采用两种昂贵的策略：1) **提前解码**，将整个视频库以原始像素格式存储，导致存储成本激增数十倍；2) **提前转码**，将视频在不同分辨率下预先存储多份，同样带来巨大的存储和计算开销。这两种方式在处理PB级视频数据时都不可行。
    - **有限的查询能力**：为了追求极致的吞吐量，许多级联系统被高度特化，仅支持简单的**时间查询**（例如，“视频中何时出现了汽车？”），而无法支持更具实用价值的**空间查询**（例如，“在画面左上角区域出现了几辆汽车？”），这极大地限制了系统的应用场景。

### 2. 方法：提出一个名为 COVA 的混合域级联分析框架，从根本上解决解码瓶颈

COVA 的核心思想是将分析任务**智能地拆分到两个域**中执行：在无需解码的**压缩域**完成大部分粗粒度的筛选和跟踪工作，仅对极少数关键帧在**像素域**进行昂贵的解码和精准分析。该框架由三大创新阶段构成，形成一个高效的分析流水线。

- **第一阶段：压缩域轨迹检测**：此阶段完全在压缩数据上操作，目标是快速定位并跟踪移动物体。
    - **轻量级移动物体检测 (`BlobNet`)**：COVA 设计了一个超轻量级的神经网络 **BlobNet**，它不读取任何像素信息，而是直接以视频的**压缩元数据**（如运动矢量、宏块类型和分区模式）作为输入，快速输出一个标记了移动物体（**Blobs**）大致位置的掩码。为了适应不同视频的特性（如镜头角度、光照），`BlobNet` 会针对每个视频进行一次性的、自动化的**即时特化训练**。
    - **高效物体跟踪**：将 `BlobNet` 在连续帧上检测到的“块（Blobs）”通过高效的跟踪算法（如SORT）连接起来，形成代表单个物体连续运动的**轨迹（Tracks）**。至此，系统已在不解码的情况下掌握了“何时、何处”有移动物体。

- **第二阶段：解码成本感知的锚点帧选择 (Decoding-Cost-Aware Anchor Frame Selection)**：此阶段的目标是从每个物体的运动轨迹中，智能地选择一个或少数几个最具代表性且**解码成本最低**的**锚帧（Anchor Frames）**。
    - **解码成本建模**：COVA利用了视频编码中**GOP**（Group of Pictures）的依赖结构。解码一个序列（GOP）中靠后的P帧或B帧，需要先解码它所依赖的所有前序帧。因此，一个帧在GOP中的位置越靠前，其解码成本越低。
    - **智能选择算法**：COVA 设计了一个贪心算法，它会为每个即将结束的运动轨迹，选择一个能够覆盖该物体、且在GOP中位置尽可能靠前（即依赖链最短）的帧作为锚点帧。这确保了所有物体都能被识别，同时将解码的总工作量降至最低。

- **第三阶段：像素域标签传播 (Pixel-Domain Label Propagation)**：这是唯一需要解码和运行高精度DNN的阶段。
    - **选择性解码与推理**：系统**仅解码**第二阶段选出的极少数锚点帧及其依赖帧，然后将这些解码后的帧送入一个高精度的DNN模型（如 YOLOv4）进行物体识别，获得准确的标签（如“汽车”）和边界框。
    - **标签关联与传播**：将DNN在锚点帧上识别出的精确物体，与第一阶段在压缩域中检测到的轨迹进行空间位置上的关联。一旦关联成功，该物体的标签就会被**传播**到其对应轨迹所覆盖的**所有帧**上。这样，COVA仅通过分析几帧就完成了对整个物体运动片段的标注。

### 3. 效果：在系统吞吐量上取得SOTA性能，同时保证了可接受的精度并扩展了查询能力

- **系统吞吐量巨幅提升**：与被解码瓶颈限制的传统级联系统相比，COVA 的端到端处理吞吐量**平均提升了 4.8 倍**。这是因为它成功地将超过 **73%** 的帧从昂贵的解码和DNN推理流水线中过滤掉，从根本上解决了解码瓶颈。

- **可接受的精度与成本权衡**：这种巨大的性能提升带来了约10-20%的“适度”准确率损失。论文论证，对于旨在快速从海量视频中进行探索性分析和获取高层洞察的应用场景，这种权衡是完全可以接受且极具价值的。因为对于回放性视频分析，提供的往往只是近似结果。

- **强大的通用性与扩展性**：
    - **原生支持空间查询**：由于COVA的框架本身就处理和保留了物体的时空轨迹信息，因此它能够**原生支持空间查询**，且与时间查询相比没有额外的性能或精度损失，显著优于现有系统。
    - **多编码器兼容**：通过敏感性分析，证明了COVA的设计理念不局限于H.264，同样适用于VP8、VP9、HEVC等其他主流的基于块的视频编码标准，展现了其框架的**通用性**。
    - **瓶颈分析**：消融实验证明，COVA的流水线设计是有效的，其新的性能瓶颈取决于视频内容，可能落在解码阶段（对于运动密集的视频）或DNN推理阶段（对于运动稀疏的视频），但整体吞吐量远高于基线系统。


## Gemel: Model Merging for Memory-Efficient, Real-Time Video Analytics at the Edge
**本文由UCLA、普林斯顿大学、微软研究院、浙江大学合作完成，发表在会议 NSDI '23。项目主页见 https://github.com/artpad6/gemel_nsdi23**

### 1. 动机：现有边缘视频分析系统在处理多模型实时任务时面临根本性的显存困境

**核心问题：** 随着摄像头和AI应用的普及，边缘计算节点（Edge Box）需要同时运行越来越多、越来越复杂的深度学习模型来进行实时视频分析。然而，这些边缘设备的GPU显存资源极其有限且增长缓慢，导致在满足**实时性、低成本和高准确性**三者之间存在根本性的矛盾。当多个模型的总大小超过GPU显存时，系统不得不频繁地在CPU和GPU之间交换模型，这一过程的巨大延迟开销导致了严重的性能瓶颈。

**现有方案的瓶颈：**

*   **时间/空间共享 (Time/Space Sharing)**：这是最直接的应对策略，即通过分时复用GPU来运行超出显存容量的模型。但其致命缺陷在于**模型加载开销巨大**。论文通过实验（Table 1）证明，将一个模型从CPU加载到GPU的时间（几十到上百毫秒）往往远超其单次推理时间（几毫秒），有时甚至高达**34倍**。对于要求每帧处理时间在33毫秒以内的实时视频流，这种延迟是不可接受的，直接导致系统**大量丢帧（高达19-84%）**，最终造成**端到端任务准确率雪崩（下降高达43%）**。
*   **模型压缩/量化 (Compression/Quantization)**：这些技术通过生成更轻量的模型来降低显存和计算开销。然而，它们通常以牺牲模型准确率为代价，且对漂移（drift）更敏感。更重要的是，确定哪种压缩程度对特定任务和部署环境是合适的，需要大量领域专家的手动调优，缺乏通用性和自动化。此外，即便压缩后，多个模型依然可能超出显存限制。
*   **主干共享 (Stem-Sharing)**：一些多任务学习系统（如Mainstream）通过共享模型**起始部分连续的层（即“主干网络”）**来节省计算。但这种方法有两个局限：1) 它要求共享的层必须是连续的；2) 视觉模型中**最占用显存的“重量级”层**（如全连接层）通常位于模型的**末端**，主干共享无法触及这些层，因此内存节省效果有限。

### 2. 方法：提出一个名为 Gemel 的系统，通过智能的“模型合并”框架来解决上述困境

Gemel的核心思想是，利用不同视觉模型间广泛存在的**架构共性**，让多个模型**共享**其内部任意位置的、架构相同的层（包括权重），从而显著降低整个工作负载的显存占用。这是一个典型的云边协同系统，将复杂的合并与训练过程置于云端，而将高效的推理执行置于边缘。

**1. 指导性观察：模型合并的理论基石**
Gemel的设计基于两大关键的经验性观察，它们将一个复杂的组合优化问题极大地简化：

*   **幂律内存分布**：在一个DNN模型中，绝大多数显存（60-91%）被极少数（约15%）的“重量级”层占据。**启发**：无需合并所有层，只需优先合并这几个“重量级”层，就能以最小的准确率风险，获得最大的显存收益。
*   **每层合并决策的独立性**：一个层能否成功合并（即重训练后准确率达标），基本上**不依赖于**其他层是否也被合并。**启发**：可以将全局优化问题分解为一系列独立的、可增量解决的决策问题，极大地降低了搜索空间的复杂度。

**2. 智能合并启发式算法**
Gemel在云端运行一套高效的启发式算法，以自动发现最佳的合并方案：

*   **收益驱动的增量式合并**：系统首先识别所有可共享的层，并根据它们能带来的**潜在内存节省量**进行降序排序。然后，Gemel**一次只尝试合并一个（或一组）收益最高的层**，并在重训练成功后，保留该合并配置，再继续尝试下一个，从而逐步、稳健地构建最终的合并方案。
*   **自适应重训练**：为了将昂贵的“试错成本”降至最低，重训练过程是自适应的。它能**提前检测成功**（在准确率接近目标时，减少训练数据以加速收敛）和**提前检测失败**（在准确率迟迟不提升时，提前中止训练），显著减少了不必要的时间开销（平均减少28%的重训练时间）。
*   **鲁棒的失败处理**：当一次合并尝试失败时，系统不会简单放弃，而是会尝试一个更保守的策略（如缩小共享范围），在“收益”和“成功率”之间做出智能权衡。

**3. 合并感知的边缘推理调度**
一旦云端找到了成功的合并方案，更新后的共享权重会被部署到边缘。边缘的推理调度器也变得更加智能：

*   它会识别出哪些模型之间共享了层，并倾向于**将共享度高的模型安排在一起连续执行**。
*   当切换模型时，它**只加载那些非共享的、独有的层**，从而将模型交换的延迟降至最低，最大化模型合并带来的性能收益。

### 3. 效果：在准确率、显存效率和系统性能上均取得SOTA表现

**端到端准确率巨幅提升**：这是Gemel最核心的贡献。与仅使用传统时间/空间共享策略的基线相比，Gemel通过减少丢帧（多处理13-44%的帧），将端到端的任务准确率**提升了8-39%**。

**显存效率显著优化**：
*   Gemel成功将工作负载的整体显存需求**降低了17.5-60.7%**，换算成绝对值高达**0.2-5.1 GB**。
*   与只能共享模型头部的“主干共享”方法（Mainstream）相比，Gemel的内存节省效果要**高出5.9-52.3%**，因为它能合并模型任意位置的、更占内存的层。
*   Gemel的合并结果非常接近理论最优值（在9.3-29.0%的差距内），展示了其启发式算法的高效性。

**强大的性能与效率**：
*   Gemel的启发式算法非常高效，**超过70%的内存节省是在最初的24-210分钟内实现的**，证明其“优先合并重量级层”的策略极为有效。
*   该系统是通用的，其效果在覆盖多种模型架构、任务类型和视频场景的**超过850个**多样化工作负载上得到了验证，展现了其强大的通用性和鲁棒性。
*   消融实验证明，其增量式合并和自适应重训练等关键设计对于实现高性能至关重要。


## Extract-Transform-Load for Video Streams
**本文由MIT CSAIL、亚利桑那大学、AWS、Intel Labs合作完成，发表在会议 PVLDB '23。项目主页见 https://github.com/ferdiko/vetl**

### 1. 动机：现有大规模视频流分析面临的困境

**问题核心：** 随着城市监控、自动驾驶等场景产生海量的视频数据流，如何经济高效地从中提取有价值的信息成为一个巨大挑战。这其中存在一个难题：**低成本、高吞吐（实时性）和高精度**难以兼得。
*   **高成本**：存储原始视频流的开销惊人（千台摄像头每月230TB，存储费6万美元/年）；在查询时对全部数据运行昂贵的CV模型更是成本高昂（百台摄像头一个月的分析成本超10万美元）。
*   **低吞-吐/高延迟**：即使在现代GPU上，先进的CV模型每秒也只能处理寥寥数帧。分析长达一年的视频数据可能需要数月时间，无法满足实时或近实时的决策需求。
*   **精度损失**：为了降低成本和延迟，最直接的方法是降低采样率或使用低质量配置，但这会严重损害分析结果的准确性。

**现有方案的瓶颈：**
*   **朴素方法**：先存储所有视频，再在查询时处理。这种方法在存储和计算上都**成本过高**，且查询**延迟巨大**，完全不实用。
*   **传统数据仓库范式**：作者提出，视频分析可以类比为数据仓库问题。视频流是“易于生产但难以查询”的原始数据，需要一个ETL过程将其转化为“易于查询”的结构化数据（如检测结果表）。作者将此过程定义为 **Video Extract-Transform-Load(V-ETL)**。然而，这个V-ETL的`Transform`步骤（即运行CV模型）本身就是成本和性能的瓶颈所在。
*   **现有内容自适应系统**：这些系统能根据视频内容的难易程度动态调整处理配置（如帧率、分辨率），在内容简单时使用廉价配置，在内容复杂时使用昂贵配置。但它们的核心假设是**计算资源是峰值配置的**，即硬件能力足以应对最坏情况下的工作负载。这在追求低成本的V-ETL场景下不成立，这些系统在资源受限时**无法保证吞吐量**，会导致数据积压和系统崩溃。

### 2. 方法：提出一个名为 Skyscraper 的系统，通过预测性规划与反应式执行结合的框架，实现低成本、高吞吐的V-ETL

Skyscraper旨在**预算受限**的硬件上，通过智能地**自适应调优**CV工作流的“旋钮(knobs)”（如帧率、模型精度、图像分块等），在保证数据流不积压（吞吐量）的前提下，**最大化分析结果的质量**。这是一个结合了离线学习和在线决策的复杂系统。

**1. 核心理念：解耦预测与执行，结合长期规划与短期反应**
Skyscraper认识到，精确预测未来某一时刻视频的具体内容（如“3小时后下午2:15:30会有一群人经过”）是不可能的。但预测**未来一段时间内不同类型内容的分布**（如“未来一天中，有10%的时间是交通高峰期”）是可行的。

*   **离线学习阶段 (Offline Learning Phase)**：
    *   **内容分类**：系统利用少量无标签数据，通过K-Means聚类，自动将视频流划分为几个具有代表性的“内容类别”（如：交通稀疏、交通正常、交通拥堵）。分类的依据是不同“旋钮”配置在这些内容上产生的质量向量，确保同一类别内的内容对于所有配置都表现出相似的质量。
    *   **模型训练**：训练一个简单的前馈神经网络来**预测在未来一个规划周期（如两天）内，每种内容类别出现的频率**。
*   **在线摄取阶段 (Online Ingestion Phase)**：
    *   **预测性旋钮规划器 (Predictive Knob Planner)**：该组件**定期运行**（如每两天一次）。它利用预测模型得到的未来内容分布频率，通过一个**线性规划**求解器，为每个内容类别**静态地分配一个最优的“旋钮”配置组合**。目标是在满足总预算和吞吐量约束下，最大化期望的全局质量。这本质上是一个**资源分配**问题。
    *   **反应式旋钮切换器 (Reactive Knob Switcher)**：该组件**高频运行**（如每两秒一次）。它负责**实时地**：1) 快速判断当前流入的视频帧属于哪个内容类别（通过比较当前配置产生的质量与各类别中心的距离）；2) 查找规划器为该类别分配好的旋钮配置；3) 结合当前**缓冲区使用情况和云端资源**，做出最终的执行决策，以确保系统不掉队。

**2. 资源管理：本地计算、缓冲区与云爆发的协同**
Skyscraper明确地管理三种资源，以应对工作负载的波动：
*   **本地计算集群 (Local Compute Cluster)**：用户根据预算配置的廉价、永远在线的计算资源，是处理视频流的主力。
*   **视频缓冲区 (Video Buffer)**：当本地资源无法实时处理（如遇到复杂内容需要昂贵配置时），视频帧会被暂时存入缓冲区，等待后续处理。
*   **云爆发 (Cloud Bursting)**：当缓冲区即将溢出时，系统会将部分工作负载“爆发”到云端进行处理，以保证吞吐量。Skyscraper的规划器会智能地预算和使用云端额度，避免过早耗尽。

### 3. 效果：在成本节约、质量和鲁棒性上均取得SOTA性能

**成本-质量权衡巨幅优化**：
*   与采用静态最优配置的基线相比，Skyscraper在各种真实世界工作负载（交通监控、疫情安全检测、社交媒体情感分析）上均展现了**显著更优的成本-质量曲线**。
*   在达到与静态基线和Chameleon（一个带缓冲区的适配版）相同的质量水平时，Skyscraper的**成本降低了高达8.7倍**。

**强大的资源利用与鲁棒性**：
*   通过消融实验证明，**缓冲区和云爆发的结合**是实现最佳性能的关键。在面对短时高峰时，带宽限制使得云爆发效果不佳，此时缓冲区起主导作用；在面对长时高峰时，缓冲区会很快耗尽，此时云爆发成为关键。Skyscraper能智能协同两者，应对不同模式的负载尖峰。
*   其**预测性规划器**和**反应式切换器**的开销极低，对在线性能影响可忽略不计。
*   预测模型的准确性很高，且系统对预测误差不敏感。即使在长达8天的预测窗口下性能有所下降，其结果依然远超基线，证明了其框架的**鲁棒性**。

**通用性与实用性**：
*   Skyscraper的框架是**通用**的，不依赖于特定的CV模型或任务。用户只需定义工作流的DAG、可调的“旋钮”以及一个返回**质量指标**的函数即可。
*   它解决了V-ETL这一类重要但之前未被明确定义的问题，为大规模、低成本视频数据仓库的构建提供了第一个实用且高效的解决方案。

## GRACE: Loss-Resilient Real-Time Video through Neural Codecs
**本文由芝加哥大学、斯坦福大学、微软、NVIDIA合作完成，发表在会议 USENIX NSDI '24。项目主页见 https://uchi-jcl.github.io/grace.html**

### 1. 动机：现有实时视频抗丢包技术面临的困境

**问题核心：** 在实时视频通信（如视频会议、云游戏）中，由于对延迟的极高要求，无法通过重传恢复丢失的数据包。现有技术在应对网络丢包时，难以在**低延迟、高画质和高流畅度**之间取得理想的平衡。
*   **高延迟/卡顿**：为了保证数据完整性，传统方法或等待重传，或在丢包后请求全新的关键帧，这些都会导致明显的视频卡顿和延迟。
*   **画质下降**：为了降低丢包影响，一些方法以牺牲画质为代价。例如，增加过多的冗余（FEC）会占用带宽，导致基础画质变差；将一帧切成独立小块以进行错误隐藏，会降低压缩效率。
*   **恢复效果差**：当丢包发生时，传统的错误隐藏技术由于缺乏编码器的“指导”，只能进行“盲猜”，恢复出的画面往往充满马赛克和伪影，体验不佳。

**现有方案的瓶颈：**
*   **前向纠错 (Forward Error Correction, FEC)**：这种方法在发送端添加冗余包。其核心缺陷是**需要精确预测未来的网络丢包率**。如果冗余加少了，丢包一多，视频流就会直接崩溃；如果冗余加多了，会持续浪费大量带宽，导致整体画质下降。
*   **错误隐藏 (Error Concealment, EC)**：这种方法在接收端尝试“猜测”和填补丢失的画面部分。其核心缺陷是**编解码器解耦**。编码器为了追求极致压缩，去除了所有冗余，没有为解码器的“猜测”提供任何额外信息。这导致解码器只能基于有限的上下文进行恢复，效果很差。
*   **分离设计范式的局限**：现有系统普遍遵循经典的“信源-信道分离”设计原则，即“压缩”和“抗丢包”是两个独立的、串联的模块。这种分离设计导致了上述问题，因为压缩过程丢弃了对恢复有用的信息，而抗丢包过程又不了解视频内容的具体重要性。

### 2. 方法：提出一个名为 GRACE 的系统，通过端到端联合训练的神经网络编解码器，将抗丢包能力内化到编解码器自身

GRACE 的核心思想是**打破“压缩”和“抗丢包”的分离设计**，通过一种新颖的训练范式，让视频编解码器本身就“学会”如何在不完美的网络中进行鲁棒的通信。这是一个算法与系统深度结合的解决方案。

**1. 核心理念：通过模拟丢包进行端到端的联合训练**
GRACE 的基石是**神经网络视频编解码器 (NVC)**。其强大的抗丢包能力来源于一个创新的训练过程：
*   **训练时模拟丢包 (Simulated Packet Loss)**：在训练流程中，GRACE 在编码器输出特征张量后，会人为地、**随机地将其中一部分元素置为零（随机掩码）**，然后再送入解码器进行重建。
*   **联合优化 (Joint Optimization)**：这个“模拟丢包”操作强迫编码器和解码器进行协同进化：
    *   **编码器学会“智能冗余”**：为了让解码器在信息不全时仍能恢复图像，编码器必须学会将一个重要的视觉信息**分散地、冗余地**编码到特征张量的多个位置。
    *   **解码器学会“鲁棒重建”**：解码器在训练中见过了成千上万种“残缺”的输入，学会了如何从不完整的线索中**推理和重建**出最可能的原始图像。

**2. 关键系统设计：保障鲁棒性发挥的工程实现**
为了将训练好的模型落地为可用的实时系统，GRACE 设计了多个关键机制：
*   **可逆随机分包 (Reversible Randomized Packetization)**：设计了一种巧妙的分包策略，它将编码后的特征张量元素随机打乱再分装进网络包。这使得真实世界中**“丢掉一个数据包”**的效果，与训练时**“随机置零一部分元素”**的效果完美等价，确保了模型的学以致用。
*   **乐观编码与动态状态重同步 (Optimistic Encoding & Dynamic State Resync)**：为了解决丢包导致的编解码器参考帧不一致问题，GRACE 提出了一种创新的协议：
    *   **乐观编码**：编码器总是假设网络良好，持续编码下一帧，保证了视频流的**流畅性**。
    *   **动态重同步**：当解码器检测到丢包并使用了不完整的参考帧后，会立刻通过一个**轻量级的反馈**通知编码器。编码器在本地**极速地**重新计算出与解码器同步的参考帧，从而在不中断视频流的情况下，快速消除错误传播。
*   **效率与码率控制 (GRACE-Lite & Bitrate Control)**：为了在普通 CPU 和移动设备上实现实时运行，提出了轻量化的 **GRACE-Lite** 版本。同时，通过重用运动估计结果和预训练多个残差编码器，实现了对输出码率的快速、精确控制。

### 3. 效果：在流畅度、画质和用户体验上均取得SOTA性能

**流畅度与可靠性巨幅提升**：
*   与最先进的 FEC 方案 (Tambur) 相比，在真实网络轨迹下，GRACE **将无法解码的帧数减少了 95%，将视频卡顿时长减少了 90%**。
*   其创新的状态同步协议避免了传统方法因等待重传或请求I帧而造成的巨大延迟。

**有丢包情况下的画质优势**：
*   GRACE 的画质随丢包率增加而**“优雅地”下降**，而不是像 FEC 那样出现“悬崖式”崩溃。
*   与基于神经网络的错误隐藏方案相比，GRACE 在 20%-80% 的丢包率下，视频质量指标 (SSIM) **高出 0.5-4 dB**。
*   在包含 240 名参与者和 960 次评分的用户研究中，GRACE 的**平均主观意见分 (MOS) 比所有基准方案高出 38%**。

**无丢包情况下的性能对标**：
*   在网络状况良好时，GRACE 的压缩效率与 H.265 等主流传统编解码器**相当**。
*   其**GRACE-Lite**版本在 iPhone 14 Pro 等移动设备上能达到超过 25fps 的实时编码速度，证明了其框架的**实用性**。

**通用性与开创性**：
*   GRACE 的框架是**通用**的，可以与任何拥塞控制算法集成。
*   它**首次**系统性地提出并实现了一种全新的抗丢包范式，即通过联合训练将网络适应性内化到编解码器中，为解决实时通信中的长期痛点问题提供了第一个高效且优雅的解决方案。


## Edge-assisted Adaptive Configuration for Serverless-based Video Analytics
**本文由清华大学与华为合作完成，发表在会议 IEEE ICDCS '23，代码见：https://github.com/STAR-Tsinghua/ServerlessVideoAnalytics。**

### 1. 动机：现有视频分析系统面临成本与准确率的两难困境

**问题核心：** 现代视频分析严重依赖于计算密集型的深度神经网络，导致运行成本高昂。传统的资源配置方式难以在**低成本、高准确率和动态适应性**之间取得理想的平衡。
*   **成本高昂/资源浪费**：传统方案通常采用“峰值配置”策略，即为云端服务器或边缘设备配置足以处理最复杂视频场景的资源。然而，视频内容的复杂性是动态变化的，在大部分时间里，这些资源处于闲置状态，造成了巨大的成本浪费。
*   **精度无法保证**：如果为了省钱而采用低配方案（如使用小模型、低帧率），当视频内容变得复杂时（如物体快速移动），分析准确率会急剧下降，无法满足应用需求。
*   **配置僵化**：现有的视频分析系统通常采用一套固定的配置（模型、帧率、计算资源）来处理所有视频流，无法根据实时变化的视频内容和网络状况进行动态调整，缺乏灵活性和效率。

**现有方案的瓶颈：**
*   **云端集中处理**：将所有原始视频流发送到云端进行分析，虽然算力强大，但网络传输开销大，且对所有帧进行昂贵的DNN推理，成本极高。
*   **边缘独立处理**：完全在边缘设备上运行分析，受限于边缘设备有限的计算和存储能力，难以部署高精度的大型DNN模型。
*   **分离的优化目标**：现有研究通常只关注优化其中一个或两个配置“旋钮”（如只调整帧率或模型），而忽略了视频配置（模型、帧率）和平台配置（计算资源）之间的复杂相互作用。这种分离的优化方式无法找到全局最优的成本-准确率平衡点。

### 2. 方法：提出一个边缘辅助的无服务器视频分析框架，通过算法动态联合调整配置，实现成本最小化

该框架的核心思想是**将边缘的轻量级追踪与云端的重量级检测相结合，并利用一个智能算法来动态指挥整个系统**，从而在满足精度要求的前提下，最大限度地降低由无服务器平台产生的计算成本。

**1. 核心理念：边缘-云协同与动态配置**
框架通过将任务智能地分配给边缘和云端，并动态调整三类关键“旋钮”来实现优化：
*   **分工合作**：
    *   **边缘服务器**：负责轻量级任务，包括视频解码、**目标跟踪**，以及运行核心的**自适应配置算法**。
    *   **云端无服务器平台 (Serverless Platform)**：只在被选取的关键帧上执行昂贵的 **DNN 对象检测**。
*   **动态联合优化**：算法的核心是联合调整三个关键配置：
    *   **DNN 模型**：在多个不同大小和精度的模型（如 YOLOv5x, s, n）之间选择。
    *   **帧率**：决定发送多少关键帧到云端进行检测。
    *   **计算资源**：在无服务器平台（如AWS Lambda）上为函数选择最优的内存大小，以间接控制CPU算力和成本。

**2. 关键系统设计与算法：保障成本-精度最优**
为了将这一理念落地，系统设计了清晰的架构和高效的算法：
*   **系统架构**：设计了一个清晰的**数据流**（视频帧流动）和**控制流**（配置决策流动）分离的边缘-云协同架构。边缘服务器作为“大脑”，指挥云端无服务器函数的工作模式。
*   **问题建模**：将问题形式化为一个**带约束的非线性优化问题**，目标是在满足**最低准确率要求 (`A`)** 的约束下，**最小化计算成本 (`Fc`)**。
*   **自适应配置算法**：由于问题是NP-hard的，作者提出了一种基于**马尔可夫近似** 的高效启发式算法。该算法通过迭代搜索，智能地探索（模型、帧率、内存）的组合空间，快速收敛到近似最优解。它能够根据视频内容变化，在每个决策周期输出一组新的最优配置。

### 3. 效果：在保证精度的同时，计算成本显著降低

**成本与精度优势**：
*   与两种固定配置的基线方案相比，在满足相同目标准确率的前提下，该自适应框架**将计算成本降低了高达 77%**。
*   与另一个先进的自适应视频分析系统 **Glimpse** 相比，该框架在成本上**降低了 9% 到 26%**，并且精度表现更稳定。

**动态适应性验证**：
*   实验清晰地展示了框架的自适应行为：当视频中物体运动速度加快时，算法会自动**提高发送到云端的帧率**以保证准确率；当画面静止时，则**降低帧率**以节省成本。
*   尽管配置（帧率、成本）在动态变化，最终的分析精度始终被**稳定地控制在用户设定的目标值附近**。

### PacketGame: Multi-Stream Packet Gating for Concurrent Video Inference at Scale
**本文由中国科学技术大学完成，发表在会议 ACM SIGCOMM '23，代码见：https://github.com/yuanmu97/PacketGame。**

### 1. 动机：发现被忽视的解码瓶颈，限制大规模并发能力

**问题核心：** 在大规模视频分析系统中，尽管推理模型和帧过滤等环节已被高度优化，但一个被长期忽视的环节——**视频解码**——成为了整个系统的端到端并发性能瓶颈。
*   **并发能力受限/解码成为瓶颈**：作者在运营一个超千路摄像头的真实系统中发现，经过优化的推理模块能支持高达3015路并发，而解码模块（即便使用12核CPU）却只能支持35路。原因是解码器必须处理每一路视频流的所有数据包，而后续的推理模型仅需处理被过滤后不到2%的关键帧。这导致了严重的“头重脚轻”问题。
*   **现有优化方案的局限性**：
    *   **服务器端帧过滤 (On-server Filtering)**：发生在解码之后，无法减轻解码器的负担，治标不治本。
    *   **端侧过滤/专用压缩 (On-camera Filtering/Specialized Compression)**：需要修改摄像头固件或使用特定的视频编码器，缺乏对现有商用设备和离线视频的通用性。
    *   **模型加速 (Model Acceleration)**：只优化了流水线的末端，反而加剧了解码环节的瓶颈效应。
*   **协调机制缺失**：在处理上百路并发视频流时，如何将有限的解码资源动态分配给“最有价值”的视频流是一个巨大挑战。简单的轮询调度策略效率极低，与理想的全局调度相比，并发能力相差近两个数量级。

### 2. 方法：在解码前引入“数据包门控”框架 (PacketGame)，智能过滤数据包

该框架的核心思想是在视频被解码之前，引入一个全新的“数据包门控”阶段。它通过一个轻量级、有理论保证的算法，仅根据编码后的数据包元数据（如包大小、帧类型）来智能判断是否值得解码，从而直接从根源上解决解码瓶颈。

**1. 核心理念：解码前过滤与全局协调**
*   **攻击瓶颈**：将过滤决策前移至解码器之前。这不仅减少了解码的计算开销，也自然地减少了后续所有阶段的负载。
*   **通用设计**：该方法无需修改摄像头或视频源，作为一个纯软件插件即可部署，完美兼容现有的商用摄像头和视频文件。
*   **在线决策与优化**：将问题建模为一个在线学习问题，目标是在满足固定的解码预算下，最大化解码的“有效”数据包数量，从而保证高推理精度。

**2. 关键系统设计与算法：PacketGame 的三大模块**
为了实现这一理念，PacketGame 设计了一个由三个模块构成的闭环决策系统：
*   **上下文预测器 (Contextual Predictor)**：这是框架的“大脑”，一个超轻量级神经网络。它创新性地采用**多视角学习 (Multi-view Learning)**，将独立帧（I-frame）和预测帧（P/B-frame）的数据包大小序列作为不同视角输入，以捕捉场景复杂度与场景变化的双重信息，从而生成对数据包“必要性”的精准预测。
*   **时间估计器 (Temporal Estimator)**：作为“记忆”，它利用下游推理结果的在线反馈，采用类似**UCB (Upper Confidence Bound)** 的机制来平衡**探索与利用 (Exploration-Exploitation)**。它鼓励系统关注近期回报高的视频流（利用），同时也不会忽视长期未被检查的流（探索），以防错过突发事件。
*   **组合优化器 (Combinatorial Optimizer)**：作为“指挥官”，它负责全局资源分配。它将问题转化为一个近似背包问题，并采用一个高效的**贪心算法**，根据每个数据包的“置信度/解码成本”性价比进行排序，在总解码预算内做出全局最优的选择。

### 3. 效果：解码开销大幅降低，端到端并发能力显著提升

**解码效率与并发增益**：
*   与原始工作负载相比，在保证90%以上推理精度的前提下，PacketGame 能够节省 **52.0% 到 79.3%** 的解码计算成本。
*   在相同的解码资源下，PacketGame 实现了 **2.1 到 4.8 倍** 的端到端并发能力提升。

**与现有技术的互补性与优越性**：
*   实验中最具说服力的对比显示，在一个已经使用 TensorRT 进行模型加速的系统上（并发能力为30路），再集成服务器端帧过滤技术 InFi，并发能力仅提升至35路（瓶颈仍在解码）。
*   而将 InFi 替换为 PacketGame 后（`TRT+PacketGame`），系统的并发能力**上升至 169 路**，提升了近5倍。这证明了PacketGame在解决并发瓶颈上的独特价值和与现有技术的强大互补性。

### InFi: End-to-end Learnable Input Filter for Resource-efficient Mobile-centric Inference
**本文由中国科学技术大学完成，发表在会议 ACM MobiCom '22，代码见：https://github.com/yuanmu97/infi。**

### 1. 动机：现有输入过滤器设计缺乏理论指导且适用性有限

**问题核心：** 在移动端AI推理中，大量输入数据是冗余的（如不含人脸的图片、结果无变化的视频帧），直接对这些数据进行昂贵的推理计算造成了巨大的资源浪费。然而，现有的输入过滤方法存在三大瓶颈。
*   **缺乏理论指导**：设计过滤器更像是一门“手艺”，依赖于反复试验。开发者无法预先判断一个AI任务是否“值得”被过滤，导致开发成本高且效果无法保证。
*   **特征表示能力不足**：现有方法大多依赖手工设计的低级特征（如像素差、边缘信息）或通用的预训练模型特征。这些特征并非为“过滤”这个特定任务而优化，导致在多样化的AI任务中判别能力不足，过滤精度和效率低下。
*   **适用场景局限**：大多数过滤器仅为特定数据类型（如图像）或特定部署模式（如云端卸载）设计，无法处理移动应用中常见的文本、传感器信号等数据，也无法支持模型分区（Model Partitioning）等更先进的部署模式。

### 2. 方法：提出首个端到端可学习的通用输入过滤框架 (InFi)

该框架的核心思想是，将输入过滤视为一个可学习的、端到端优化的任务。它不仅为“何时该过滤”提供了理论依据，还通过一个统一的框架解决了“如何高效过滤”的问题，极大地扩展了过滤技术的适用范围。

**1. 核心理念：理论指导与端到端学习**
*   **提出“可过滤性”理论 (Filterability)**：首次从计算学习理论出发，通过比较推理模型与其过滤器的“假设族复杂度”，为判断一个AI任务是否适合被过滤提供了理论指导，避免了盲目开发。
*   **端到端可学习设计**：抛弃手工或固定特征，InFi的整个过滤模型（从特征提取到决策）都是为了“过滤”这个最终目标而协同优化的。这使得它能为每个具体任务学习到最具区分度的特征表示。
*   **统一SKIP与REUSE**：巧妙地将“跳过型”过滤（SKIP）和“重用型”过滤（REUSE）统一到一个基于孪生网络（Siamese Network）思想的框架下，用一套模型解决了两种主流的过滤需求。

**2. 关键系统设计与算法：InFi 的三大模块**
*   **多模态特征网络 (Multi-modal Feature Networks)**：作为框架的“感知层”，InFi设计了一系列轻量级的特征提取器，使其成为首个原生支持**图像、视频、音频、文本、传感器信号**以及**模型中间层特征图**六种模态的过滤框架，极大地增强了通用性。
*   **任务无关的分类器 (Task-agnostic Classifier)**：作为“决策核心”，它接收特征网络提取的嵌入向量，并作为一个**可学习的距离度量器**，计算出输入的冗余度分数。其结构通用，但参数会根据具体任务自适应优化。
*   **双模式端到端训练 (Dual-mode End-to-end Training)**：作为“学习引擎”，InFi根据过滤模式采用不同的训练策略：
    *   **SKIP模式**：视为一个**二元分类**问题，使用**交叉熵损失**进行训练。
    *   **REUSE模式**：视为一个**度量学习**问题，使用对比损失（Contrastive Loss）进行训练，学习一个高效的度量空间。

### 3. 效果：过滤效率和适用性全面领先，显著提升系统性能

**过滤效率与性能增益**：
*   与原始工作负载相比，在保证90%以上推理精度的前提下，InFi可为视频分析应用带来高达 **8.5倍** 的吞吐量提升，并节省 **95%** 的网络带宽。
*   其自身的计算和能耗开销极低，在移动设备上的推理延迟仅为标准轻量级模型MobileNetV1的 **12-25%**。

**与现有技术的互补性与优越性**：
*   在与多种基线方法（FilterForward, Reducto, FoggyCache）的对比中，InFi在所有12个测试任务上均取得了显著更优的**准确率-过滤率**权衡曲线。
*   其优越性根源在于**端到端学习到的定制化特征**。在许多基线方法因特征不适用而完全失效的任务上（如人脸检测、文本情感分析），InFi依然能保持高效过滤。
*   得益于其通用设计，InFi是首个能成功应用于**模型分区**部署模式的输入过滤器，展现了其在支持未来移动AI架构上的独特价值。


### MadEye: Boosting Live Video Analytics Accuracy with Adaptive Camera Configurations
**本文由普林斯顿大学、莱斯大学完成，发表在会议 USENIX NSDI '24，代码见：https://github.com/michaeldwong/madeye。**

### 1. 动机：现有视频分析系统忽视了摄像头方向这一关键因素

**问题核心：** 现有视频分析系统将摄像头视为一个被动的、固定的传感器，所有优化都集中在如何更高效地处理**已被捕获**的视频数据。然而，摄像头捕捉到的内容本身（即其方向、焦距）对最终分析的准确率有着决定性的影响，这一巨大的优化潜力被完全忽视了。主动调整摄像头方向面临三大瓶颈。
*   **变化快难预测**：最佳的摄像头方向（能带来最高准确率的视角）变化极快，85%的变化在1秒内就会发生。依赖历史数据进行预测的传统方法（如多臂老虎机）因其滞后性而完全失效。
*   **需求多样难统一**：不同的AI任务（如检测车辆 vs. 计数行人）、不同的模型、不同的物体，对“最佳方向”的定义天差地别。一个固定的方向或一种简单的调整策略，无法同时满足复杂工作负载下的多样化需求。
*   **搜索空间巨大**：摄像头所有可能的方向（旋转、俯仰、变焦）构成了一个巨大的搜索空间，而其中的“最优解”既稀疏又转瞬即逝。盲目或暴力搜索在实时应用中是完全不可行的。

### 2. 方法：提出首个通过自适应摄像头配置提升分析精度的协同框架 (MadEye)

该框架的核心思想是，将摄像头从被动的传感器转变为主动的“智能体”，通过一个在摄像头和服务器之间高效协同的系统，实时地探索并选择能最大化整个工作负载分析精度的视角。

**1. 核心理念：经验观察指导与“近似模型+高精度模型”协同架构**
*   **经验观察指导**：系统设计并非盲目进行，而是基于对视频数据的三个关键经验性观察：（1）最佳方向在空间上是缓慢移动的；（2）高价值方向在空间上是聚集的；（3）相邻方向的准确率变化趋势是高度相关的。这些观察为设计高效的本地搜索算法提供了理论依据。
*   **端到端协同优化**：MadEye是一个完整的端到端系统。服务器负责高精度推理和模型训练，而摄像头负责快速探索和初步筛选，二者紧密协作，共同优化“从捕捉到分析”的整个流程。
*   **“近似模型+高精度模型”架构**：巧妙地将系统分工。摄像头上运行超轻量级的**近似模型**，其唯一任务是在极短时间内快速评估大量潜在方向的价值；服务器则保留**高精度模型**，只对近似模型筛选出的最有价值的图像进行精确打击，从而实现资源效率和准确率的平衡。

**2. 关键系统设计与算法：MadEye 的三大模块**
*   **近似模型与知识蒸馏 (Approximation Model & Knowledge Distillation)**：作为框架的“感知层”，MadEye不为每个任务设计专用模型，而是统一采用一个轻量级**物体检测器**作为通用近似模型。该模型通过**知识蒸馏**从服务器端的重型模型中学习其“偏好”，并能通过**持续学习**不断自适应场景变化。
*   **快速本地搜索算法 (Fast Local Search Algorithm)**：作为“决策核心”，这是在摄像头本地运行的高效算法。它在每个时间步内，动态维护一个由连续方向组成的“探索区域”，并基于近似模型的快速评估结果，智能地用更有潜力的新方向替换掉表现最差的旧方向。
*   **服务器-摄像头协同训练与推理 (Server-Camera Co-training and Inference)**：作为“学习和执行引擎”，服务器不仅负责对摄像头筛选出的最佳图像进行最终的高精度推理，还承担着训练、更新和下发近似模型的任务，形成了一个完整的“探索-验证-学习”闭环。

### 3. 效果：分析准确率和资源效率全面领先，开辟优化新维度

**准确率与资源效率增益**：
*   与最优的固定摄像头方案相比，在消耗同等资源的前提下，MadEye可将视频分析任务的中位数准确率提升 **2.9% - 25.7%**。
*   反之，要达到与MadEye相同的准确率，需要部署 **2-3.7倍** 数量的固定摄像头，这意味着MadEye能带来巨大的硬件和带宽成本节省。
*   其自身算法在边缘设备（Jetson Nano）上的开销极低，完全满足实时运行要求。

**与现有技术的互补性与优越性**：
*   在与多种摄像头自适应基线方法（Panoptes, MAB, Tracking）的对比中，MadEye的准确率取得了全面、压倒性的领先（例如，比MAB高**5.8倍**）。
*   其优越性根源在于其决策是基于**当前场景的实时内容**，而非依赖过时的历史数据或简单的规则。这使其能更快速、更精准地响应动态变化的场景。
*   MadEye可以和现有的其他视频流优化技术（如Chameleon）**完美互补**，在其他技术节省资源的基础上，进一步“免费”地提升系统准确率。

### NeuroScaler: Neural Video Enhancement at Scale
**本文由韩国科学技术院 (KAIST) 完成，发表在会议 ACM SIGCOMM '22，代码见：https://github.com/kaist-ina/neuroscaler-public。**

### 1. 动机：神经网络视频增强技术因成本过高而难以规模化

**问题核心：** 在直播服务器端通过AI技术提升视频质量前景广阔，但现有方案在实际部署中面临三大严峻挑战，导致其计算成本高昂，无法支持商业级规模（如Twitch的10万路并发流）。
*   **昂贵的端到端增强流程**：完整的“解码-AI推理-重编码”流程中，**AI超分辨率**和**高分辨率视频编码**这两个环节都极其消耗计算资源，共同构成了难以逾越的成本壁垒。
*   **低效的选择性增强策略**：为了降低成本，现有方法尝试只对部分“重要”帧进行增强。然而，如何“选择”这些帧本身就是一个难题。最先进的方案（如NEMO）需要运行昂贵的预推理来辅助决策，这并未从根本上解决问题，且不适用于实时直播。
*   **无效的资源调度**：当面临大规模并发流时，传统的基于流的负载均衡策略完全失效。由于不同视频内容的增强难度和收益各异，这种粗粒度的调度会导致严重的GPU资源浪费和负载不均，无法最大化集群的整体质量增益。

### 2. 方法：提出首个可扩展的、端到端协同优化的神经增强框架 (NeuroScaler)

该框架的核心思想是，通过一系列紧密耦合的系统设计和算法创新，对视频增强的全流程进行深度优化，从而在数量级上降低成本，实现真正的可扩展性。

**1. 核心组件 I：Anchor 调度器 (Anchor Scheduler)**
*   **零推理锚点帧选择 (Zero-Inference Anchor Frame Selection)**：作为框架的“决策大脑”，此模块是NeuroScaler的关键创新。它深刻洞察到，无需运行AI模型，仅利用视频**编码器自身产生的元数据**（如帧类型和残差信息），就足以高效、准确地预估增强每一帧的潜在收益。该算法完全在CPU上运行，速度极快，解决了选择性增强的决策瓶颈。
*   **锚点感知的资源管理 (Anchor-Aware Resource Management)**：作为框架的“指挥中心”，它采用**中心化的全局视角**，从所有直播流中统一挑选出“性价比”最高的增强任务（即锚点帧）。随后，它以“任务”为粒度，而非以“流”为粒度，将这些锚点帧增强任务精细地分配给集群中的各个GPU，从而实现了最优的负载均衡和资源利用。

**2. 核心组件 II：Anchor 增强器 (Anchor Enhancer)**
*   **混合视频编码 (Hybrid Video Encoding)**：作为框架的“效率核心”，这一设计巧妙地**绕过**了最昂贵的高清视频重编码环节。它并不生成一个全新的高清视频流，而是将**原始的低分辨率视频流**与**被单独编码为高质量图片的高清锚点帧**打包在一起。真正的视频合成在客户端完成，服务器端的编码开销因此被降至最低。
*   **GPU上下文切换优化 (GPU Context Switching Optimization)**：这是一个关键的底层工程优化。通过模型预编译和内存预分配等技术，NeuroScaler将多模型、多任务间的GPU切换开销从秒级降至毫秒甚至微秒级，确保了在高并发场景下GPU推理的高效性。

### 3. 效果：吞吐量与成本效益实现数量级突破，为大规模部署铺平道路

**端到端性能增益**：
*   与对每帧进行增强的基线方案相比，在达到同等画质的前提下，NeuroScaler将系统吞吐量提升了 **10倍**，而处理每路流的成本则惊人地降低了 **22.3倍**。
*   相较于传统的选择性增强方案，成本也降低了 **3.0 - 11.1倍**，吞吐量提升了 **2.5 - 5倍**。
*   案例研究表明，在一个Twitch规模（10万路并发流）的场景中部署NeuroScaler，每小时的运营成本约为**7,898美元**，完全在商业可接受范围内。

**各组件的独立贡献**：
*   实验通过“逐一添加优化”的方式清晰地证明了NeuroScaler每个组件的价值：**GPU上下文优化**是系统能够实时运行的基础；**混合编码**将吞吐量提升了2.16倍；而**零推理锚点选择**则在其基础上将吞吐量再次提升了2.30倍。
*   这证明了NeuroScaler的成功并非来自单一的奇技淫巧，而是一整套协同工作的系统性解决方案的胜利。


### NEMO: Enabling Neural-enhanced Video Streaming on Commodity Mobile Devices
**本文由韩国科学技术院 (KAIST) 完成，发表在会议 ACM MobiCom '20，代码见：https://github.com/kaist-ina/nemo。**

### 1. 动机：AI视频增强在移动设备上面临严峻的性能与功耗瓶颈

**问题核心：** 将AI超分辨率技术移植到移动端以提升视频体验，是技术发展的必然趋势。然而，这一美好的愿景被移动设备固有的三大资源限制所阻碍，使得传统的“每帧计算”方案完全不可行。
*   **计算性能瓶颈**：移动设备GPU的算力远逊于桌面级显卡。实测表明，在高端手机上运行超分辨率模型，处理速度仅为5-10fps，远低于实时播放所需的流畅度，导致视频严重卡顿。
*   **能耗与电池寿命危机**：AI推理是耗电大户，其功耗比普通视频播放高出5.9至18.1倍。这意味着手机的续航时间会从十几个小时锐减到不足两小时，对移动用户来说是不可接受的。
*   **设备过热问题**：持续的高强度计算导致手机迅速升温，表面温度可轻易超过40°C。这不仅会引起用户不适，还可能触发系统降频保护，进一步恶化性能，甚至存在低温烫伤的风险。

### 2. 方法：提出基于“选择性计算与结果复用”的服务器-客户端协同框架 (NEMO)

NEMO的核心思想是，不再对每一帧都进行昂贵的AI计算，而是通过服务器与客户端的智能协同，只对视频中一小部分精心挑选的帧（**锚点, Anchor Points**）在移动端执行AI增强，然后高效地复用这些高质量结果来“重建”其余所有帧。

**1. 设计核心 I：如何选择锚点 (Anchor Point Selection) 与保证质量**
*   **服务器端离线分析**：NEMO在视频分发前，于服务器端进行一次性的离线分析。它通过一个高效的贪心算法，迭代地选择出能最大化质量收益的“锚点”帧。
*   **基于质量损失的优化目标**：该算法的目标是，在保证增强后的视频质量与“理想状态”（即每帧都进行增强）相比，其质量损失（Cache Erosion）被严格控制在一个预设的阈值内（如PSNR下降<0.5dB）的前提下，使用的锚点数量最少。
*   **生成缓存配置文件 (Cache Profile)**：最终选定的锚点信息被编码成一个极小的“缓存配置文件”，随视频一同下发。这个文件就是客户端执行智能决策的“说明书”。

**2. 设计核心 II：如何复用结果 (SR-Integrated Codec)**
*   **深度集成的解码器**：NEMO并未将AI模块作为播放器的上层应用，而是将超分辨率功能**深度集成到底层视频解码器（libvpx）内部**。这种设计让它能访问到最精细的帧间依赖信息。
*   **利用编码信息进行重建**：当解码一个非锚点帧时，该集成解码器会：
    1.  从缓存中查找最近的高质量锚点帧作为参考。
    2.  利用当前帧在原始视频流中的**运动矢量 (Motion Vector)** 和 **残差 (Residual)** 信息。
    3.  通过轻量级的运动补偿和残差叠加操作，快速、精确地“重建”出当前帧的高质量版本。
*   **高效的客户端执行流程**：锚点帧的处理（AI推理）在GPU上进行，而非锚点帧的重建则在CPU上高效完成。整个过程被封装在解码器API内部，对上层应用透明。

### 3. 效果：首次在商用移动设备上实现可用的、节能的实时AI视频增强

**性能、能耗与温度的全面优化**：
*   **实时性能达成**：与“每帧增强”方案相比，NEMO将视频处理吞吐量平均提升了 **11.5倍**，在多种商用手机上均实现了远超30fps的实时处理能力。
*   **能耗与发热显著降低**：NEMO将AI增强带来的额外能耗降低了 88.6%，并将设备表面温度成功控制在**35°C以下**的用户舒适区内。
*   **极高的计算节省**：在保证高质量的前提下（平均PSNR损失仅0.41dB），NEMO仅对视频中 1.8%至9.7% 的帧进行了昂贵的AI计算，其余超过90%的帧都通过高效复用解决。

**对用户体验质量 (QoE) 的实际提升**：
*   在真实的移动网络环境下，相比仅使用传统自适应码率（ABR）技术的播放器，NEMO通过利用终端算力，能够将用户的综合体验质量（QoE）平均提升31.2%。
*   这一提升是“免费”的，因为它不消耗任何额外的网络带宽。反之，它也可以在提供相同体验质量的前提下，为用户节省 18.3% - 22.1%的数据流量。


### **Optimizing Video Selection LIMIT Queries With Commonsense Knowledge**
**本文由密歇根大学、南加州大学及麻省理工学院等机构完成，发表在会议 VLDB 2024，代码见：https://github.com/wenjiah/video-commonsense-optimization。**

### 1. 动机：视频查询的“不可能三角”：速度、成本与准确性的困境

**问题核心：** 在大规模视频数据库中，高效的内容检索面临一个根本性的“不可能三角”困境，即难以同时实现查询速度快、索引成本低和结果100%准确，现有方法往往只能顾此失彼。
*   **速度瓶颈：朴素扫描的漫长等待**：在查询时逐一扫描视频，并调用耗时的AI模型进行检测，是保证准确性的最直接方法。然而，其极低的效率导致用户需要忍受分钟级甚至小时级的等待，这在交互式应用中是完全不可接受的。
*   **成本黑洞：全量索引的巨额开销**：为了追求极致的查询速度，可以预先处理所有视频的每一帧来构建完整索引。这种“暴力美学”将计算成本前置，造成了惊人的资源浪费，因为99%以上的索引数据可能永远不会被查询到。
*   **准确性陷阱：稀疏索引的信息鸿沟**：作为一种折中，廉价的稀疏索引（只处理少量帧）虽然解决了成本问题，但其“有损”特性会导致大量目标对象被遗漏。若无智能策略填补这一信息鸿沟，查询的准确性和完整性将无法得到保障。

### 2. 方法：PAINE框架——以“常识模型”为核心的智能查询优化

PAINE的核心方法论是，承认并接纳索引的不完备性，将优化的重心从“构建完美的索引”转移到“构建一个能理解不完美索引的智能大脑”上。这个大脑就是**常识模型**，它负责在查询时进行概率推理，指导系统将有限的计算资源用在“刀刃”上。

#### **设计核心 I：知识建模——如何量化并学习“常识”**

PAINE提出了两种截然不同但目标一致的路径来构建常识模型，以适应不同场景的需求。

**路径A：基于公共知识的推导模型 (Model without Videos)**

此路径专为冷启动或无法获取大量视频训练数据的场景设计，完全依赖公开数据和数学推导。其核心是分步估算条件概率 `P(目标O | 观测Li)`。

1.  **估算先验概率 `P(Object)`**：
    *   **原理**：一个对象在视频中出现的概率，与其在人类社会中的普遍性/关注度正相关。
    *   **实现**：使用**维基百科页面浏览量**作为代理指标。将一个对象的日均浏览量，除以所有可检测对象中的最大浏览量，进行归一化，得到其先验概率。例如，`P(汽车)`会远高于`P(盔甲)`。

2.  **估算对象关联度 `P(O ∩ Li) / P(O ∪ Li)`**：
    *   **原理**：两个对象的语义关联性越强，它们在同一个视频中共同出现的可能性就越大。
    *   **实现**：利用**知识图谱（如ConceptNet）的词嵌入**。计算两个对象词向量的**余弦相似度**，来近似它们的关联度。作者巧妙地用这个相似度来估算Jaccard系数（交并比），而非直接估算联合概率，这样可以有效排除对象自身流行度的干扰，更纯粹地反映“绑定”关系。

3.  **计算多元条件概率 `P(O | Li)`**：
    *   **挑战**：当观测列表`Li`包含多个对象时，直接计算高维联合概率是极其困难的。
    *   **解决方案**：采用经典的**朴素贝叶斯（Naïve Bayes）**思想。
        *   **核心假设**：在给定目标`O`存在的条件下，观测到的各个对象`Li,j`的出现是**相互独立的**。
        *   **公式转换**：通过贝叶斯定理，`P(O|Li) ∝ P(O) * P(Li|O)`，而`P(Li|O)`被近似为`Π P(Li,j|O)`的连乘。这样，一个复杂的高维问题被分解为多个易于计算的二元概率问题。
    *   **分母`P(Li)`的处理**：为估算`Li`中所有对象同时出现的联合概率，作者引入了**Fréchet不等式**，它能根据两两对象间的联合概率，给出一个联合概率的上下界。最终取其平均值作为近似。

**路径B：基于视频数据的学习模型 (Model with Videos)**

此路径是性能更优的选择，它通过深度学习直接从数据中归纳出常识。

1.  **模型架构：微调的BERT (Fine-tuned BERT)**
    *   **原理**：将概率预测看作一个**回归任务**，利用强大的预训练语言模型BERT来学习从“观测对象”到“目标对象存在概率”的复杂映射。
    *   **选择理由**：BERT的Transformer架构能处理变长输入序列，其自注意力机制能捕捉对象间复杂的上下文关系（超越条件独立性假设），且其预训练权重已包含大量通用世界知识，非常适合迁移学习。

2.  **输入构建**：
    *   **格式**：将输入构造成一个特殊的序列：`[CLS] obj_1 obj_2 ... [SEP] target_obj`。
    *   **细节**：观测对象`obj_i`按其在视频中首次出现的顺序排列，以保留潜在的时间信息。`[CLS]`和`[SEP]`是BERT的特殊标记，前者用于汇总序列信息进行回归预测。

3.  **训练过程**：
    *   **数据生成**：从已有的视频对象统计数据中，为每个视频列表生成大量的正样本（从列表中任取一个对象作目标，其余为观测，label=1）和负样本（从列表外随机选取一个对象作目标，label=0）。
    *   **微调**：在预训练的BERT模型上添加一个回归层，使用生成的训练数据和均方误差（MSE）损失函数进行端到端的微调。模型学习的目标是使其预测值无限接近于真实标签（0或1）。

#### **设计核心 II：查询执行——如何应用“常识”进行智能调度**

PAINE设计了一套高效的在线查询流程（Algorithm 1），将常识模型的能力转化为实实在在的性能提升。

1.  **快速路径：直接命中检查**
    *   查询开始时，系统首先扫描廉价的稀疏索引。如果目标对象恰好在某个视频的索引中被记录，该视频被直接视为命中，加入结果集。

2.  **智能预测与排序**
    *   对于所有未直接命中的视频，查询优化器调用已准备好的**常识模型**。
    *   模型为每个视频的稀疏索引`Li`，计算出其包含目标对象`O`的概率`P(O|Li)`。
    *   所有待处理的视频，将根据这个概率分数，被放入一个**优先级队列**中，进行**从高到低的排序**。

3.  **优先级驱动的顺序验证**
    *   系统严格按照上述排序，从概率最高的视频开始，逐一进行**完整的、昂贵的对象检测**。
    *   每当一个视频被验证确实包含目标对象，它就被加入结果集。
    *   这个过程持续进行，直到结果集的大小满足用户指定的`LIMIT`数量，查询便立即终止。

**方法的优势**：这个流程确保了计算资源被最优地利用。系统不会在那些常识判断为“极不可能”的视频上浪费时间，而是集中火力攻击最有可能的目标，从而以最小的代价，最快的速度完成查询任务。

### 3. 效果：以低索引成本实现高速、精确的视频查询

**性能、成本与准确性的全面胜利**：
*   **查询处理效率的指数级提升**：与传统的扫描方法相比，PAINE通过智能排序，将需要进行完整AI检测的视频数量平均减少了 **97.79%**。这意味着在保证结果100%准确的前提下，查询速度实现了数量级的提升。
*   **保持最低的索引构建成本**：PAINE的卓越性能是在仅构建廉价“稀疏索引”的基础上实现的，成功地以极低的固定成本，换取了接近“全量索引”的查询效率，打破了成本与速度的传统矛盾。
*   **专用知识模型的优越性与鲁棒性**：实验证明，经过领域数据微调的专用BERT模型，其性能远超直接使用的通用大语言模型（GPT-3.5）。同时，整个系统在不同查询类型、不同索引密度和不同`LIMIT`值下均表现出强大的鲁棒性。


### **Reducto: On-Camera Filtering for Resource-Efficient Real-Time Video Analytics**
**本文由加州大学洛杉矶分校（UCLA）完成，发表在顶级网络会议 SIGCOMM 2020，代码见：https://github.com/reducto-sigcomm-2020/reducto**

### 1. 动机：实时视频分析面临的资源与准确率挑战

**问题核心：** 实时视频分析流水线面临着高昂的资源需求（网络和计算）与严格的查询准确率要求之间的根本性矛盾。
*   **资源开销巨大：** 传统的流程中，摄像头持续将视频流传输至后端服务器，消耗大量网络带宽。服务器随后对每一帧运行基于神经网络（NN）的分析模型，这带来了巨大的计算开销。当部署大规模摄像头网络时，这些开销会被急剧放大。
*   **现有过滤方法的局限性：** 为了提升效率，现有系统普遍采用帧过滤策略。然而，这些策略在应用于资源受限的摄像头时存在显著不足：
    *   **基于NN的过滤方法：** 即使是轻量级的神经网络（如Tiny YOLO），在普通摄像头的低功耗CPU上运行速度也远低于实时要求，无法部署。
    *   **二进制分类模型：** 此类模型仅判断帧内是否存在目标物体，但当物体持续存在而查询结果（如数量、位置）不变时，会错失大量的过滤机会。
    *   **基于帧差和静态阈值的方法：** 此类方法无法适应视频内容和查询类型的动态变化，使用固定的阈值会导致查询准确率无法得到保证，经常出现不可接受的准确率下降。

### 2. 方法：Reducto框架——一个用于在端过滤的动态自适应系统

Reducto提出了一种服务器与摄像头协同工作的框架，通过动态调整过滤决策，在满足用户指定的准确率的前提下，最大化地在摄像头端过滤冗余视频帧。其核心在于一个分阶段、闭环的自适应流程。

#### **阶段一：服务器端离线分析与特征选择 (Server-side Offline Profiling)**

此阶段在系统运行前由服务器一次性完成，旨在为不同查询类别选择最相关的低成本信号。

1.  **数据收集与关联分析**：
    *   服务器获取一段代表性视频，并执行两项操作：(1) 运行高准确率的后端模型（如YOLO）以获得每帧的基准查询结果；(2) 计算视频中每对连续帧之间多种低级特征（如`Pixel`差异、`Edge`差异、`Area`差异）的差异值。
2.  **最佳特征确定**：
    *   通过量化分析，服务器评估每个低级特征的变化与基准查询结果变化之间的相关性。
    *   **核心发现 (Observation 1)**：对于一个给定的查询类别，其“最佳特征”是相对稳定的。例如，`Area`特征（运动区域面积变化）最适合**计数查询**，因为它对物体数量变化最敏感；而`Edge`特征（边缘轮廓变化）最适合**边界框检测查询**，因为它能捕捉到物体微小移动导致的位置变化。
    *   最终，服务器为每种支持的查询类别预先配置一个最佳特征。

#### **阶段二：模型驱动的动态阈值调整 (Model-driven Dynamic Threshold Tuning)**

当一个新查询（包含准确率目标）到达时，系统进入此阶段，为该查询动态地生成一个过滤决策模型。

1.  **初始数据采集与在线训练**：
    *   查询开始时，摄像头首先将一小段未经任何过滤的原始视频帧（作为初始训练集）发送给服务器。
    *   服务器对这段视频进行详尽分析：
        *   将视频划分为多个小的**段 (segments)**，如每秒一段。
        *   对每个段，服务器计算一个**29维的特征差异向量**（假设30fps），该向量描述了这一秒内视频内容的详细变化模式。
        *   服务器对每个段模拟所有可能的过滤阈值，并找到一个能在**满足用户指定准确率**的前提下，实现最大过滤率的**最佳阈值**。
        *   这样，服务器就获得了大量（场景特征向量, 最佳阈值）的数据点。
    *   服务器使用 **K-Means 聚类**算法，将具有相似场景特征向量的数据点聚集在一起。

2.  **轻量级模型生成与下发**：
    *   聚类的结果被编码成一个非常小巧的**哈希表**。此哈希表中的每一项代表一类场景，其**键 (Key)** 是该场景的聚类中心和范围，其**值 (Value)** 是对应于该场景的平均最佳过滤阈值。
    *   服务器将这个哈希表模型发送给摄像头。

#### **阶段三：在端过滤与在线模型更新 (On-Camera Filtering and Online Model Retraining)**

摄像头接收到模型后，进入常态化的过滤工作，并具备应对未知情况的能力。

1.  **在端过滤执行**：
    *   摄像头持续地为每个视频段计算其特征差异向量。
    *   使用该向量在本地的哈希表中进行查询，找到匹配的场景条目，从而获得一个**动态的过滤阈值**。
    *   摄像头使用此阈值对该段内的每一帧进行判断，过滤掉帧间差异小于阈值的帧，仅将关键帧发送至服务器。

2.  **在线模型重训练**：
    *   **触发条件**：当摄像头计算出的特征向量在哈希表中找不到匹配项时，系统推断遇到了一个未知的视频场景。
    *   **安全保障措施**：为保证准确率，摄像头立即**暂停过滤**，并将该段的**所有原始视频帧**发送给服务器。
    *   **模型更新**：服务器接收到这些新数据后，将其加入训练集，**重新运行K-Means聚类以更新模型**。更新后的哈希表被发送回摄像头，摄像头随后恢复过滤工作。这个闭环机制确保了系统能够持续学习和适应新场景。

### 3. 效果：在低成本硬件上实现高效且准确率有保障的实时视频分析

**Reducto在效率、资源节省和准确率保证方面取得了显著成果：**
*   **高过滤率与资源节省**：实验表明，Reducto 能够过滤掉 **51% 至 97%** 的视频帧。这直接带来了 **21%-86% 的网络带宽节省** 和 **50%-96% 的后端计算开销降低**。同时，查询响应时间也降低了 22%-26%。
*   **严格的准确率保障**：在所有实验中，Reducto 始终能够满足用户设定的查询准确率目标，其最终达成的准确率一直高于指定的阈值。
*   **在资源受限设备上的可行性**：Reducto 的在端过滤算法在树莓派 Zero 上能够以 **47.8 fps** 的速度运行，充分证明了其在低成本、低功耗摄像头上部署的可行性。
*   **与现有系统的比较优势**：与基于二进制分类的过滤系统（如 FilterForward）相比，Reducto 过滤了显著更多的帧。与视频压缩（CloudSeg）和配置自适应（Chameleon）等互补技术相比，Reducto 在降低后端计算负载和网络带宽方面表现出独特的优势。

### **TASM: A Tile-Based Storage Manager for Video Analytics**
**本文由华盛顿大学（University of Washington）与微软完成，发表在顶级数据库会议 ICDE 2021，代码见：https://github.com/uwdb/TASM**

### 1. 动机：现有视频存储限制了分析效率

**问题核心：** 现代视频分析系统在处理大规模视频数据时，其性能瓶颈常常出现在最基础的存储层。传统的视频存储方式将视频作为一个不可分割的整体文件，这严重限制了数据访问的灵活性，导致了巨大的资源浪费。

*   **缺乏空间随机访问能力：** 当查询只关心视频帧中的一小部分区域时（例如，画面底部的车辆），现有系统仍然必须从磁盘读取并解码**整个视频帧**。这种“读了再扔”的模式消耗了大量的 I/O 带宽和 CPU 计算资源，尤其是在处理高分辨率视频时。
*   **查询类型的普遍性：**
    *   **子帧选择查询**（例如，“裁剪出所有蜂鸟的特写”）非常普遍，其瓶颈完全在于存储和解码。
    *   **对象检测查询**（例如，“找到视频中所有的汽车”）为了避免对每一帧都运行昂贵的深度学习模型，通常会先用一个廉价的过滤器进行全盘扫描。这个扫描阶段的性能同样受限于全帧解码的效率。
*   **现有优化方法的不足：** 大多数现有工作都集中在优化解码后的分析阶段（例如，使用更快的模型），而忽略了优化解码本身这一根本性问题。这使得存储层成为了一个被忽视的性能瓶颈。

### 2. 方法：TASM框架——一个基于图块的自适应视频存储管理器

TASM 提出了一种全新的视频存储管理范式，它利用现代视频编码标准（如 HEVC/H.265）中的“图块 (Tiles)”特性，将视频帧在物理上分割成多个可独立解码的矩形区域。通过智能地管理这些图块的布局，TASM 实现了对视频内容的空间随机访问，从而在查询时只读取和解码与查询相关的部分。其核心在于一套多场景、自适应的平铺策略。

#### **阶段一：物理存储设计与基础能力**

此阶段定义了 TASM 的核心数据结构和物理存储模型，是所有上层策略的基础。

1.  **基于图块的数据存储**：
    *   **核心思想**：不再将视频存为单个文件，而是将每个视频帧分割成多个图块，每个图块作为一个独立的、可解码的迷你视频流进行存储。
    *   **布局类型**：TASM 支持两种图块布局：
        *   **均匀布局**：将帧划分为固定的网格（如 2x2, 4x4）。简单但效率较低。
        *   **非均匀布局**：**TASM 的核心优势**。根据视频中对象的位置和分布，“量身定制”图块的边界，使其能紧密地包围感兴趣的对象。
    *   **粒度控制**：在非均匀布局中，TASM 还能控制图块的粒度，既可以使用少量大图块（粗粒度），也可以使用大量小图块（细粒度），并在两者之间进行权衡。
2.  **语义索引 (Semantic Index)**：
    *   为了快速定位图块，TASM 维护一个 **B-Tree 聚簇索引**，键为 `(video, label, time)`。
    *   该索引的叶子节点存储了对象精确的**边界框坐标**，以及一个**指向包含该对象的、已经编码好的图块文件的指针列表**。这是连接逻辑查询（“找车”）和物理读取（“读 tile_3.mp4”）的关键桥梁。
3.  **时序管理 (Sequence of Tiles - SOT)**：
    *   由于对象会移动，一个固定的图块布局不能适用于整个视频。TASM 将视频在时间上划分为多个段（SOT），每个 SOT 可以拥有自己独立的图块布局。实验表明，将 SOT 的长度设为与视频的 GOP（Group of Pictures）长度一致时，能在性能和存储开销间取得最佳平衡。

#### **阶段二：智能平铺策略 (Tiling Strategies)**

TASM 的“大脑”在于其能够根据可用信息的多少，采用不同的策略来决定如何以及何时进行平铺。所有决策都基于一个精确的成本模型：`Cost = β * (解码像素数) + γ * (解码图块数)`。

1.  **策略一：基于感兴趣区域的初始平铺 (ROI Tiling for New Videos)**
    *   **场景**：当一个全新的视频被加载，没有任何先验知识时。
    *   **方法**：为了加速后续的首次全盘扫描，TASM 首先运行一个**计算成本极低**的算法（如背景差分、运动向量分析）来识别**感兴趣区域 (ROI)**。然后，它围绕这些 ROI 创建一个初始的非均匀图块布局。
    *   **价值**：这个“有根据的猜测”能有效过滤掉大量静态背景，在解码阶段就实现初步的数据削减，尤其适合在摄像头等边缘设备上部署。

2.  **策略二：基于完整信息的静态优化 (Optimization with Known Queries & Objects)**
    *   **场景**：在理想化的“开卷考试”模式下，即提前知道所有查询和所有对象的位置。
    *   **方法**：TASM 直接采取最优策略。对视频的每个 SOT，它只考虑一种候选布局：一个围绕**所有将被查询的对象**而创建的**非均匀、细粒度**的布局。同时，它会使用成本模型进行预判，如果平铺带来的收益不大（例如，对象过于密集），则放弃平铺，保持原样。

3.  **策略三：基于增量学习的动态优化 (Optimization with Unknown Queries & Objects)**
    *   **场景**：这是最现实、最核心的场景。系统在运行中持续接收未知的查询。
    *   **方法**：TASM 采用了一种受数据库**在线索引**和**后悔最小化 (Regret Minimization)** 思想启发的增量式算法。
        *   **观察与学习**：每当一个新查询到来，TASM 会更新其“已见对象集”和“候选布局集”。
        *   **累积后悔值**：TASM 会“反思”刚刚的查询，并为每个候选布局计算一个“后悔值”，该值量化了“如果当初用了这个布局，能比现在好多少”。这个值会在整个查询历史中不断累积。
        *   **成本感知的决策**：一旦某个候选布局的**累计后悔值**超过了**执行物理重组（重新编码图块）的成本**，TASM 就会触发一次物理布局的更新，将视频的对应部分更新为这个更优的布局。这个闭环机制确保了 TASM 能够持续学习和适应新的查询模式。

### 3. 效果：显著提升视频查询性能并优化存储访问

**TASM 在多个维度上验证了其设计的有效性，取得了显著的性能提升：**
*   **查询加速效果显著**：
    *   对于**子帧选择查询**，TASM 的布局优化平均能带来 **51%** 的性能提升，在最佳情况下可达 **94%**。
    *   对于**对象检测任务**，TASM 的 ROI 平铺策略能将预处理阶段的吞吐量提升高达 **2倍**，有效缓解了解码瓶颈，且不影响模型准确率。
*   **智能决策的有效性**：
    *   基于**后悔最小化**的增量式优化算法被证明是极其有效的。在多变的查询负载下，它始终能收敛到接近最优的布局，并能智能地避免在不适合平铺的密集场景下进行昂贵且无效的重组。
*   **物理布局设计的优越性**：实验一致表明，**非均匀布局**在性能和视频质量（PSNR > 40dB）上均显著优于传统的均匀网格布局。将图块布局的更新周期（SOT）与 GOP 同步，是在性能和存储开销之间的最佳权衡点。
*   **端到端性能提升**：在一个更接近真实应用的宏观基准测试中，TASM 将整个工作流的运行时间减少了 **12% 至 39%**。


### **SEIDEN: Revisiting Query Processing in Video Database Systems**
**本文由佐治亚理工学院与 Adobe 完成，发表在顶级数据库会议 VLDB 2023，代码见：https://github.com/georgia-tech-db/seiden_submission**

### 1. 动机：轻量级“代理模型”在视频分析中的核心假设已失效

**问题核心：** 现有先进的视频数据库管理系统 (VDBMSs) 通常依赖于一个核心假设：使用一个轻量级但不太准确的“代理模型”（Proxy Model，如 ResNet-18）来快速过滤掉大量无关帧，然后再将少量候选帧交给一个重量级但准确的“预言机模型”（Oracle Model，如 Mask R-CNN）进行精细分析。然而，计算机视觉领域的飞速发展，特别是 YOLOv5 等新一代对象检测模型的出现，已经彻底颠覆了这一假设。

*   **性能差距消失：** 新的“预言机模型”（如 YOLOv5s）的推理速度已经与甚至超过了传统的“代理模型”（如 ResNet-18）。例如，在某些配置下，YOLOv5s 比 Mask R-CNN 快 **11.2倍**，同时准确率（mAP）还更高。这意味着，为了获得一个不太准确的代理结果而先运行一个代理模型，已经不再是一个明智的时间投资。
*   **现有系统的根本性缺陷：** 像 TASTI、BlazeIt 等基于代理模型的系统，其架构设计的根基已经动摇。继续沿用“代理-预言机”流水线会带来不必要的开销和潜在的准确率损失。
*   **新机遇：** “预言机模型”变得又快又准，这为我们提供了一个重新设计 VDBMS 查询处理流程的绝佳机会，即**直接、但有策略地使用预言机模型**。

### 2. 方法：SEIDEN框架——一个摒弃代理模型、直接利用预言机模型的全新系统

SEIDEN 提出了一种全新的 VDBMS 架构，它完全摒弃了代理模型的概念，转而直接、智能地在视频帧的子集上运行轻量级的预言机模型（如 YOLOv5s）。其核心在于一个两阶段的、利用视频时序连续性的查询处理流程，在查询执行期间通过“探索-利用”策略进行自适应采样。

#### **阶段一：索引构建 (Index Construction) - 一次性的查询无关预处理**

此阶段在视频首次加载时执行一次，旨在构建一个轻量级、查询无关的“锚点”索引，为后续所有查询服务。

1.  **锚点采样 (Anchor Sampling)**：
    *   SEIDEN 不会处理视频的每一帧，而是仅从视频中采样一小部分 **I-frames**（关键帧）。这些被选中的 I-frame 被称为**锚点 (anchors)**。
    *   **选择 I-frame 的原因**：(1) I-frame 可以独立解码，节省了解码时间；(2) I-frame 通常位于场景切换或视频内容的显著变化处，本身就包含了丰富的信息。
2.  **直接运行预言机模型**：
    *   SEIDEN **直接在这些锚点帧上运行预言机模型**（如 YOLOv5s），获取高精度的查询结果（例如，对象计数、对象是否存在等）。
3.  **构建索引**：
    *   将（帧ID，预言机模型结果）作为键值对存入一个简单的哈希索引中。这个索引的构建成本非常低，因为它只处理了极少数帧。相比于需要处理所有帧的 TASTI 等系统，SEIDEN 的索引构建速度要快几个数量级。

#### **阶段二：查询执行 (Query Execution) - 自适应的探索与利用**

当一个新查询（如聚合查询或检索查询）到达时，SEIDEN 利用第一阶段的锚点索引，并启动一个自适应的采样流程。

1.  **视频分段 (Video Segmentation)**：
    *   利用第一阶段选出的锚点，SEIDEN 在逻辑上将整个视频划分为多个连续的**段 (segments)**，每个段的起止点都是锚点。
2.  **基于多臂老虎机 (MAB) 的自适应采样 (MABSAMPLING)**：
    *   SEIDEN 将查询采样问题建模为一个**多臂老虎机 (Multi-Armed Bandit, MAB)** 问题，其中每个视频段就是一个“臂”。
    *   **目标**：在有限的采样预算内，决定接下来应该从哪个视频段中采样帧，以最高效地提升查询的整体准确率。这是一个经典的**探索-利用 (exploration-exploitation)** 权衡问题。
    *   **奖励函数 (Reward Function)**：奖励的设计与查询类型相关。对于聚合查询，奖励与该段内查询结果的**方差**成正比，即结果越不确定、波动越大的段，奖励越高，越值得“探索”。对于检索查询，奖励与查询谓词是否满足直接相关。
    *   **UCB 算法**：SEIDEN 使用**上置信界 (Upper Confidence Bound, UCB)** 算法来选择下一个要采样的段（臂）。UCB 算法会综合考虑一个段的**预期回报（利用）**和其**被采样的次数（探索）**，优先选择那些预期回报高或探索不足的段。
3.  **标签传播 (Label Propagation)**：
    *   在通过 MAB 采样并获得了一系列帧上的精确预言机模型结果后，视频中仍有大量帧未被采样。
    *   为了给这些未采样的帧赋一个代理值（proxy label），SEIDEN 采用了一种极其简单但高效的方法：**基于时间的线性插值**。
    *   例如，如果第 10 帧的汽车数量是 5，第 20 帧是 10，那么未被采样的第 12 帧的汽车数量就被近似估计为 6。这种方法利用了视频内容的**时序连续性**。实验证明，这种简单的插值方法在性能和准确率上均优于使用像素距离或 ResNet 特征距离等更复杂的方案。
4.  **近似查询处理 (Approximate Query Processing, AQP)**：
    *   最后，SEIDEN 将所有采样帧上的**精确预言机标签**和所有未采样帧上的**插值代理标签**打包，交给现成的 AQP 算法（如 BlazeIt 或 SUPG）来计算最终的查询结果，并确保满足用户指定的准确率或置信度要求。

### 3. 效果：在保证准确率的同时，实现远超现有系统的查询速度

**SEIDEN 在查询速度和准确率方面均取得了突破性的成果，证明了其架构的优越性：**
*   **数量级的性能提升**：在多个真实世界的数据集和多样的查询类型上，SEIDEN 的平均查询速度比 TASTI-PT 等先进的 VDBMS **快 6.6 倍**。
*   **高效的索引构建**：由于只在少量 I-frame 上运行预言机模型，SEIDEN 的索引构建时间比 TASTI-PT 平均快 **87倍**，这使得其一次性成本几乎可以忽略不计。
*   **高查询准确率**：尽管 SEIDEN 的核心是一种采样方法，但得益于直接使用高精度的预言机模型和智能的 MAB 采样策略，它在聚合查询和检索查询（F1-score）上的准确率均能达到甚至超过基于代理模型的系统。
*   **设计的简洁与高效**：SEIDEN 证明了，在现代硬件和模型下，摒弃复杂的代理模型，回归到利用视频的**时序连续性**这一基本属性，并结合经典的**探索-利用**框架，是构建下一代高效视频数据库系统的更优路径。



### **TVM: A Tile-based Video Management Framework**
**本文由北京理工大学与上海交通大学完成，发表在顶级数据库会议 VLDB 2023，代码见：https://github.com/InkosiZhong/TVM**

### 1. 动机：视频分析中的“解码开销”与“查询无关”优化是两大被忽视的瓶颈

**问题核心：** 现有先进的视频数据库管理系统 (VDBMSs) 在追求查询效率时，往往陷入两个误区：(1) **忽略视频解码开销**：它们大多假设视频帧已被解码并加载到内存，主要关注如何减少昂贵AI模型（DNN）的调用次数，而忽略了在海量数据背景下，解码本身就是一个巨大的性能瓶颈。(2) **依赖查询历史**：存储优化框架通常依赖历史查询负载来对视频进行分区或缓存，这使得它们在面对新的、未知的查询类型时性能不佳，缺乏普适性。

*   **解码成为新瓶颈：** 随着摄像头分辨率和帧率的提升，视频数据量急剧膨胀（一个720p摄像头每天产生超20GB数据）。将所有数据解码后存入内存是不现实的。因此，**如何只解码必要的数据**，成为与“减少DNN调用”同等重要的问题。
*   **查询无关优化的缺失：** 一个理想的VDBMS应该能在数据首次入库时，就利用视频**内容本身的语义信息**进行一次性优化，从而为未来所有**未知类型**的查询提供普服性的加速，而不是被动地等待和学习查询模式。
*   **新机遇：** HEVC等现代视频编码标准原生支持**基于图块（Tile-based）的独立编解码**，这为在空间维度上实现子帧级别（sub-frame）的精准数据访问提供了可能。结合视频内容的语义信息，我们可以设计出一种全新的、同时优化解码和检测的查询处理流程。

### 2. 方法：TVM框架——一个同时优化解码与检测的、查询无关的图块化管理系统

TVM提出了一种新颖的、基于图块的视频管理框架，其核心是在视频入库时，构建一个**查询无关的、层次化的语义索引**。该索引不仅能指导未来的查询过滤掉大量无关数据，还能从物理上指导视频以一种最优化的分块方式进行存储和解码。

#### **阶段一：索引构建 (Index Construction) - 一次性的、语义驱动的预处理**

此阶段在视频首次加载时对每个GOP（一组图片）执行一次，旨在构建一个能最小化未来平均检索成本的物理和逻辑索引。

1.  **轻量级ROI提取与初始布局 (Lightweight ROI Extraction & Initial Layout)**：
    *   TVM首先使用**极轻量级**的**背景减除**算法，在不依赖任何DNN的情况下，快速定位视频帧中可能包含物体的**感兴趣区域 (ROI)**。
    *   然后，围绕这些ROI生成一个非常**精细的初始分块布局**，目标是让每个ROI都落在一个独立的小图块内，最大限度地隔离无效背景。

2.  **语义近似与伪标签生成 (Semantic Approximation & Pseudo-label Generation)**：
    *   为了在不运行昂贵目标检测模型的情况下获取语义信息，TVM采用了一种巧妙的替代方法：
    *   **嵌入与聚类**：使用一个轻量级的**嵌入神经网络**（如ResNet-18）将每个包含ROI的图块转换为一个低维特征向量（embedding），然后对这些向量进行**聚类**。
    *   **生成伪标签**：将同一簇（cluster）内的图块赋予相同的“伪标签”**。这样，TVM就在不知道“这是车”的情况下，知道了“这些图块的内容是相似的”，从而获得了对视频内容语义分布的近似描述。

3.  **最优解码布局计算 (Optimal Layout Calculation)**：
    *   这是TVM的核心创新。其目标是找到一个**能最小化未来平均解码成本**的分块布局。
    *   **精确的成本模型**：TVM建立了一个精确的**随机访问解码成本模型**，该模型综合考虑了**解码器初始化开销、依赖解码开销、目标图块解码开销、解码器释放开销**这四个关键因素。这个模型揭示了分块粒度与解码效率之间的复杂权衡关系。
    *   **高效的最优解搜索**：基于成本模型和伪标签，TVM将寻找最优布局的问题建模为一个**区间动态规划**问题。通过高效的DP算法，它能从众多候选布局中，找到那个在统计意义上使未来平均检索成本最低的“最优布局”。

4.  **层次化索引与图块化存储 (Hierarchical Indexing & Tiled Storage)**：
    *   **层次化索引**：TVM的最终索引是**两层**的。**外层**使用“最优布局”，用于指导高效的视频**解码**；**内层**保留“初始布局”，用于在解码后指导精确的**数据裁剪**，以优化DNN**推理**。
    *   **图块化存储**：视频最终会按照计算出的“最优布局”进行物理分割，每个图块序列被独立编码成一个视频流，并与层次化索引元数据一同存储。

#### **阶段二：查询执行 (Query Execution) - 普适性与针对性结合的加速**

当一个新查询到达时，TVM利用已构建的索引，从多个层面进行加速。

1.  **查询无关的普适性加速 (Query-independent Acceleration)**：
    *   **代理分数传播与剪枝**：利用索引中的嵌入向量，TVM可以通过高效的“传播”算法，为所有ROI图块快速估算出与查询相关的**代理分数**，从而帮助上层查询框架过滤掉大量无关图块。
    *   **层次化推理**：在解码一个根据“最优布局”划分的大图块后，TVM会利用索引中的**内层（子）布局**信息，只**裁剪**出其中真正包含ROI的、更小的子区域送入DNN进行分析，极大地减少了AI模型的计算量。

2.  **查询驱动的针对性加速 (Query-driven Acceleration)**：
    *   **定制化并行解码**：TVM会分析查询的类型。对于`Limit`查询这种具有“提前终止”特性的任务，TVM设计了一种创新的“按行并行”解码策略，避免了传统并行方法带来的大量无效解码。
    *   **资源缓存**：对于`Track`查询这种具有强烈访问局部性的任务，TVM设计了一套包含**解码器、状态、图块**的三层缓存机制，有效避免了重复的实例化、状态重置和解码操作。

### 3. 效果：在不牺牲质量和存储的前提下，实现解码与检测的双重、数量级加速

**TVM在查询性能、资源开销和普适性方面均展示了卓越的能力，实验结果有力地证明了其架构的先进性：**

*   **数量级的端到端性能提升**：在多个真实世界的数据集上，TVM的端到端查询速度比传统的基于帧的方法最高可**快5.6倍**。这种提升是解码和检测双方面的，例如在`archie`数据集上，一个聚合查询的**解码和检测时间均节省了约82%**。

*   **对各类查询的普适性加速**：
    *   **聚合查询**：效果最显著，最高取得了**5.6倍**的加速。
    *   **Limit查询**：平均加速比约**1.7倍**，在`archie`数据集上解码和检测阶段分别节省了超过41%和42%的时间。
    *   **Select查询**：平均加速比约**2倍**，在高清视频上解码和检测时间节省均超过**55%**。
    *   **Track查询**：平均加速比约**1.7倍**，同样在解码和检测阶段展现出巨大优势。

*   **极低的额外开销**：
    *   **存储开销几乎为零**：通过按面积比例分配码率，TVM分块存储后的总文件大小与原始视频相比，相对误差控制在**1%以内**。
    *   **视觉质量无损**：分块后再拼接的视频，其PSNR（峰值信噪比）均高于30dB，与原始视频**几乎没有肉眼可感知的质量差异**。

*   **核心算法的有效性验证**：
    *   **最优布局算法**：实验证明，TVM建立的解码成本模型能够准确预测实际开销（平均误差仅**0.28%**）。基于该模型计算出的“调整后布局”，其解码效率显著优于传统的“最细粒度”和“最粗粒度”布局，分别节省了18%和32%的解码时间。
    *   **层次化布局**：通过在解码后利用子布局进行精确裁剪，TVM可以在DNN推理阶段额外获得9%的性能提升，证明了为解码和推理分别优化的有效性。

*   **查询驱动优化的显著效果**：
    *   **并行解码策略**：针对`Limit`查询设计的“按行并行”策略，与传统的并行方法相比，可以额外避免约36%的无效计算浪费。
    *   **资源缓存策略**：针对`Track`查询设计的三层缓存机制（解码器、状态、图块），总共能节省超过14%的解码时间。

*   **高效的索引构建**：TVM的索引构建过程（包括ROI提取、布局计算、嵌入生成等）吞吐量极高，达到**百fps级别**（如在`amsterdam`数据集上达到579fps），完全可以满足视频数据**实时入库**的处理需求。



### **TileClipper: Lightweight Selection of Regions of Interest from Videos for Traffic Surveillance**
**本文由IIIT Delhi与Indira Gandhi Delhi Technology University for Women完成，发表在顶级系统会议 USENIX ATC 2024，代码见：https://github.com/shubhamchdhary/TileClipper**

### 1. 动机：高昂的带宽成本与边缘设备算力瓶颈亟待解决

**问题核心：** 在大规模交通监控场景中，成千上万的摄像头将视频流发送到云端服务器进行AI分析，这带来了巨大的网络带宽压力。现有的带宽节省技术通常采用两种策略，但都存在明显缺陷：(1) **智能筛选与重编码**：在摄像头端或边缘设备上运行模型，只发送包含感兴趣物体（如车辆、行人）的帧或区域。这种方法需要**视频重编码 (re-encoding)**，计算开销巨大，对于廉价的边缘设备来说难以承受。 (2) **云端增强计算**：先向云端发送低分辨率视频，由云端运行更强大的模型进行分析，必要时再请求摄像头发送高分辨率数据。这种方法将计算压力和成本转移到了云端，同样代价高昂。

*   **重编码的巨大开销：** 无论是集成到摄像头固件，还是在外部设备上进行，视频重编码都对计算能力提出了很高要求，增加了部署成本和复杂性，难以大规模推广。
*   **带宽与算力的两难：** 现有方案要么在边缘端消耗大量算力以节省带宽，要么为了节省边缘算力而消耗大量带宽和云端算力。缺乏一种**既轻量级又无需重编码**的方案。
*   **新机遇：** HEVC (H.265)等现代视频编码标准原生支持将视频帧分割成**可独立编解码的图块 (Tiles)**。这一特性为我们提供了一个全新的思路：直接在**编码后的视频流**上进行操作，通过**移除** 不包含运动物体的图块来减少数据传输量，从而**完全避免重编码**。

### 2. 方法：TileClipper框架——一个基于码率统计、无需解码的超轻量级ROI选择系统

TileClipper提出了一种全新的、在摄像头边缘侧运行的超轻量级系统，它不依赖DNN，也无需解码视频，仅通过分析**编码后视频流中每个图块的码率 (bitrate)**，就能智能地识别并只传输包含感兴趣区域（即运动物体）的图块。

#### **核心洞察：图块码率与运动物体的强相关性**

TileClipper的理论基石源于对视频编码原理的深刻洞察：
1.  **静态背景易于压缩**：对于监控视频中静止的背景（如道路、天空、建筑物），视频编码器可以非常高效地通过帧间预测进行压缩，因此这些区域对应的图块码率非常低。
2.  **运动物体难以压缩**：当有车辆或行人等运动物体出现时，会引入大量新的、不可预测的信息（残差数据）。编码器为了准确表示这些变化，必须分配更多的比特，导致这些图块的**码率显著升高**。
3.  **强相关性验证**：实验证明，单个图块的码率（文件大小）与其内部包含的运动物体数量存在**非常强的正相关性**（斯皮尔曼相关系数>0.75）。

基于这一核心洞察，TileClipper将“检测运动物体”这个复杂的计算机视觉问题，巧妙地转化为了一个简单的**“基于码率阈值的统计判断”**问题。

#### **系统架构与工作流程**

TileClipper的流程分为**摄像头/边缘端**的实时过滤和**服务器端**的一次性校准。

1.  **摄像头/边缘端：实时图块过滤 (Real-time Tile Filtering)**
    *   **①-② 视频编码**：摄像头捕获视频并使用HEVC标准将其编码为**分块视频 (Tiled Video)**。
    *   **③-④ 阈值判断与过滤**：这是TileClipper的核心。对于编码后视频流中的每一个图块，系统：
        *   **解析码率**：**无需解码**，仅通过解析视频流的元数据头，就能快速获取每个图块的码率（大小）。
        *   **阈值比较**：将该图块的当前码率与一个预先校准好的、针对该图块位置的**动态阈值 (threshold)** 进行比较。
        *   **发送或丢弃**：如果码率**高于**阈值，则认为该图块包含运动物体，将其发送到服务器；如果码率**低于**阈值，则直接**丢弃**该图块。
    *   这个过程极度轻量，可以在树莓派（Raspberry Pi）等廉价边缘设备上实时运行。

2.  **服务器端：一次性自适应阈值校准 (One-time Adaptive Threshold Calibration)**
    *   **⑤-⑥ 校准过程 (CALIBRATE)**：为了给每个图块确定一个最优的阈值，TileClipper在初始部署时需要一个短暂的校准阶段（例如30秒视频）：
        *   **数据收集**：摄像头将未经任何过滤的完整分块视频发送到服务器。
        *   **真值获取**：服务器运行一个标准的DNN模型（如YOLOv5）来识别哪些图块真正包含运动物体（**True**类别），哪些不包含（**False**类别）。
        *   **统计分析**：对于每个图块位置，分别统计其在`True`类别和`False`类别下的码率分布（如计算不同百分位数值）。
    *   **⑦ 阈值确定与反馈 (Threshold Determination & Feedback)**：
        *   **网格搜索**：服务器通过**网格搜索（grid search）**，寻找一对百分位数组合（一个来自`True`分布，一个来自`False`分布），使得以它们的中点作为阈值时，能够最大化一个综合了准确率和召回率的指标（如**F2-score**，倾向于高召回率）。
        *   **反馈**：服务器将计算出的最优百分位数配置反馈给摄像头端的TileClipper。

3.  **自适应与鲁棒性 (Adaptivity and Robustness)**
    *   **动态更新**：在实时过滤过程中，每个图块都维护着一个近期码率值的滑动窗口（例如，最近10个值）。阈值是基于这个窗口内的统计数据动态计算的，因此能够自动适应光照、天气变化。
    *   **Fallback机制**：对于校准期间几乎没有运动物体的“冷”图块，TileClipper会切换到一个基于高斯分布的**离群点检测 (outlier detection)** 策略，确保不会因为校准数据不足而完全失效。

### 3. 效果：在廉价边缘设备上实现实时过滤，大幅节省带宽且精度高

**TileClipper在带宽节省、计算开销和检测精度方面均取得了卓越的平衡，实验结果充分证明了其设计的有效性和实用性：**

*   **巨大的带宽节省**：在多个真实世界的数据集上，TileClipper平均可以减少约**22%**的数据传输量。在某些场景下，带宽节省可以超过**40%**。在真实的户外直播部署中，实现了超过**55%**的带宽节省。

*   **极低的计算开销与实时性能**：由于完全避免了解码和DNN推理，TileClipper的计算开销极低。它可以在廉价的**树莓派4B上达到22fps**，在**Jetson Nano上达到57fps**，完全满足实时视频流处理的要求。其服务器端的校准开销也非常小（例如，处理25分钟的视频仅需不到1分钟的GPU时间），远低于需要持续运行模型的竞品。

*   **高检测精度与鲁棒性**：
    *   **高精度**：在节省大量带宽的同时，TileClipper的物体检测**准确率（Accuracy）平均超过92%**，**召回率（Recall）超过90%**，与需要复杂计算的DDS等系统相当，并远超Reducto（高出12-15%）。
    *   **环境鲁棒性**：得益于其自适应阈值机制，TileClipper在不同的交通密度、光照和天气（如雨天）条件下均能保持稳定和高效的性能，且**无需在环境变化时进行重新校准**。

*   **部署简洁，易于集成**：TileCroller直接利用HEVC标准特性，无需修改摄像头固件，也无需私有解码器，使其非常容易集成到现有的监控系统中。
