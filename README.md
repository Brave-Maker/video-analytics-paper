# video-analytics-paper

## 📚 目录

- [AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics](#accdecoder-accelerated-decoding-for-neural-enhanced-video-analytics)
- [Accelerated Neural Enhancement for Video Analytics With Video Quality Adaptation](#accelerated-neural-enhancement-for-video-analytics-with-video-quality-adaptation)
- [Spatialyze: A Geospatial Video Analytics System with Spatial-Aware Optimizations](#spatialyze-a-geospatial-video-analytics-system-with-spatial-aware-optimizations)
- [Boosting Neural Representations for Videos with a Conditional Decoder](#boosting-neural-representations-for-videos-with-a-conditional-decoder)
- [Region-based Content Enhancement for Efficient Video Analytics at the Edge](#region-based-content-enhancement-for-efficient-video-analytics-at-the-edge)
- [Accelerating Aggregation Queries on Unstructured Streams of Data](#accelerating-aggregation-queries-on-unstructured-streams-of-data)
- [COVA: Exploiting Compressed-Domain Analysis to Accelerate Video Analytics](#cova-exploiting-compressed-domain-analysis-to-accelerate-video-analytics)
- [Gemel: Model Merging for Memory-Efficient, Real-Time Video Analytics at the Edge](#gemel-model-merging-for-memory-Efficient-real-time-video-analytics-at-the-edge)
- [Extract-Transform-Load for Video Streams](#extract-transform-load-for-video-streams)
- [GRACE: Loss-Resilient Real-Time Video through Neural Codecs](#grace-loss-resilient-real-time-video-through-neural-codecs)
- [Edge-assisted Adaptive Configuration for Serverless-based Video Analytics](#edge-assisted-adaptive-configuration-for-serverless-based-video-analytics)
- [PacketGame: Multi-Stream Packet Gating for Concurrent Video Inference at Scale](#packetgame-multi-stream-packet-gating-for-concurrent-video-inference-at-scale)
- [InFi: End-to-end Learnable Input Filter for Resource-efficient Mobile-centric Inference](#InFi-end-to-end-learnable-input-filter-for-resource-efficient-mobile-centric-inference)
- [MadEye: Boosting Live Video Analytics Accuracy with Adaptive Camera Configurations](#MadEye-boosting-live-video-analytics-accuracy-with-adaptive-camera-configurations)
- [NeuroScaler: Neural Video Enhancement at Scale](#NeuroScaler-neural-video-enhancement-at-scale)
- [NEMO: Enabling Neural-enhanced Video Streaming on Commodity Mobile Devices](#NEMO-enabling-neural-enhanced-video-streaming-on-commodity-mobile-devices)


## AccDecoder: Accelerated Decoding for Neural-enhanced Video Analytics

**本文由哥廷根大学完成，发表在 INFOCOM 2023。代码见：https://github.com/mi150/AccDecoder**

### 1. 需要解决的问题

低质量的视频导致视频分析准确率下降，需要对视频内容进行增强从而提高准确率。常规的方法要么花费时间过多，要么无法很好地适应视频内容的动态性，导致准确率下降。因此本文需要解决的问题是：

> 如何在保证视频分析准确率的同时，尽量降低时延，达到一个良好的权衡？

### 2. 解决方案——AccDecoder

AccDecoder 将视频分析流程分为三个步骤：

- **超分辨率增强（SR）**：为了降低延迟同时保证分析准确率，本文选择少量视频帧（称为锚帧）进行超分辨率增强。其他低分辨率帧结合锚帧、运动向量和残差合成高分辨率帧。

- **DNN 推理**：为了进一步降低延迟，仅对推理帧运行 DNN，对非推理帧则重用推理结果，并结合运动向量进行调整。

- **重用结果**：结合视频连续性和先前结果，提高效率并减少冗余计算。

### 3. 如何选择用于 SR 和推理的帧？——使用马尔可夫决策过程（MDP）

- **状态（State）**：智能体决策的依据。包括关键帧的内容特征、视频块内部的帧间差异。

- **动作（Action）**：智能体根据当前状态做出的决策。每个视频块的帧根据两个阈值分类：

  - `tr1`（SR Threshold）：用于选择锚点帧（做超分增强）  
  - `tr2`（Inference Threshold）：用于选择推理帧（执行 DNN）

- **奖励（Reward）**：智能体根据在给定时延约束下的分析准确率，来衡量策略好坏，目标是最大化准确率。


## Accelerated Neural Enhancement for Video Analytics With Video Quality Adaptation
**本文由哥廷根大学和南京大学共同完成，发表在 TON 2024 ,是 AccDecoder（INFOCOM 2023） 的扩展。**

### 1.动机： 处理来自不同摄像头或自适应编码器（如 AWStream, Pensieve, Chameleon）的异构分辨率视频流。

- **核心改进**： 在 DRL 调度器的状态 (State) 中显式加入视频块的分辨率信息。

### 2.效果： 调度器能学习针对不同分辨率块的最优阈值设置。

- **对低分辨率块**：倾向于选择更多帧做 SR（成本相对低，精度提升潜力大）。

- **对高分辨率块**：更谨慎选择 SR 帧（成本高）。

### 3.优势： 显著提升了 AccDecoder 的通用性 (Generality) 和可扩展性 (Scalability)，能更好适应实际中多源、自适应码率的视频流。


## Spatialyze: A Geospatial Video Analytics System with Spatial-Aware Optimizations  
**本文由 UC Berkeley 完成，发表在 VLDB 2024，项目主页见 https://spatialyze.github.io/**

### 1. 动机：传统视频分析系统未充分利用摄像头元数据（如地理位置、时间、姿态等）

- **问题核心**：视频实际采集自真实世界的物理空间，但现有方法大多只处理图像帧本身，忽略了空间语义与物理约束。

### 2. 方法：Spatialyze 系统通过空间元数据优化视频查询流程，共包含三个阶段：

- **构建阶段**：用户定义虚拟世界（摄像头布局、地理信息等）  
- **过滤阶段**：用户以声明式语言描述感兴趣的场景（如“在十字路口出现的人”）  
- **观察阶段**：系统执行完整分析流程，包括数据整合、视频处理、对象查询和结果输出

其中，**视频处理器模块**采用了四项空间感知优化技术：

- **Road Visibility Pruner**：跳过未覆盖目标区域的相机帧  
- **Object Type Pruner**：仅处理用户关心的目标类型  
- **Geometry-Based 3D Location Estimator**：根据边界框推算目标的 3D 物理位置  
- **Exit Frame Sampler**：基于事件预测减少不必要的帧处理

### 3. 效果：相比未优化系统，Spatialyze 在保持 97.1% 准确率的同时，提升了 5.3 倍的执行效率


## Boosting Neural Representations for Videos with a Conditional Decoder
**本文由香港科技大学、商汤科技、香港中文大学等机构合作完成，发表在 CVPR 2023。项目主页见 https://github.com/Xinjie-Q/Boosting-NeRV**

### 1. 动机：现有隐式视频表示（INRs）未能充分发挥其潜力

- **问题核心**：在解码目标视频帧时，网络内部的**中间特征与目标帧对齐不充分**，且压缩流程存在**不一致性**，导致重建质量和压缩效率受限。

### 2. 方法：提出一个通用的增强框架，通过引入条件解码器和一致性压缩来提升性能

该框架主要包含四项核心技术创新：

- **时间感知的条件解码器 (Temporal-aware Conditional Decoder)**：利用**帧索引**作为条件，通过一个无归一化的**仿射变换模块 (TAT)** 动态调整中间特征，实现特征与目标帧的精确对齐。

- **正弦NeRV类块 (Sinusoidal NeRV-like Block)**：采用 **SINE (正弦)** 作为激活函数，以生成更多样化的特征，并以更少的参数提升模型容量。

- **高频信息保留的损失函数 (High-frequency-preserving Loss)**：组合**频域L1损失**、空域L1损失和MS-SSIM损失，以更好地保留视频的边缘和纹理细节。

- **一致的熵最小化压缩 (Consistent Entropy Minimization, CEM)**：采用简单的**高斯模型**替代复杂的代理网络来估计熵，保证了训练和推理阶段的**完全一致性**，从而优化了率失真性能。

### 3. 效果：在多种视频任务上显著优于基线，并达到SOTA压缩性能

- **视频回归**：在UVG数据集上，相比基线模型平均提升 **1.2-1.7 dB PSNR**，且收敛速度更快。
- **视频压缩**：RD性能显著优于传统编解码器（H.264/H.265）和部分SOTA学习方法（DCVC），展示了极强的竞争力。
- **视频修复与插值**：在修复任务上平均带来 **1.9-4.2 dB** 的性能提升，并在插值任务上表现更优。


## Region-based Content Enhancement for Efficient Video Analytics at the Edge 
**本文由清华大学、南京大学、哥廷根大学等机构合作完成，发表在  NSDI '25。项目主页见 https://github.com/mi150/RegenHance**

### 1. 动机：现有内容增强方法在边缘视频分析中存在性能瓶颈

- **问题核心**：现有内容增强方法（如超分辨率）在应用于边缘视频分析时存在**性能与精度的根本矛盾**。对**整个视频帧进行增强**会消耗海量计算资源，导致吞吐量极低，无法实时处理；而**选择性地增强部分关键帧**又会因信息复用造成精度大幅下降，无法满足AI分析任务的苛刻要求。

### 2. 方法：提出一个名为 RegenHance 的系统，通过只增强关键区域来打破性能与精度的困境

该系统通过三大创新组件，实现了高效率与高精度的平衡：

- **基于宏块的区域重要性预测 (Macroblock-based Region Importance Prediction)**：采用视频编码中的**宏块 (`Macroblock`)** 作为基本单元，通过一个超轻量级预测模型，在原始低质量视频上**快速且准确地定位**出对分析精度提升最大的关键区域。

- **区域感知的打包增强 (Region-aware Enhancement)**：将识别出的零散、不规则的关键区域高效地**“拼接打包”**成一个或多个紧凑的矩形张量。该过程被建模为一个二维装箱问题，只对打包后的小尺寸张量进行增强，极大降低了计算开销。

- **基于配置文件的执行规划 (Profile-based Execution Planning)**：通过**离线剖析**各组件（解码、预测、增强、分析）在不同负载下的性能，在线为整个分析流水线动态生成最优执行计划，确保CPU/GPU资源被充分利用，最大化端到端吞吐量。

### 3. 效果：在精度和吞吐量上全面超越SOTA方法，实现显著性能提升

- **精度与吞吐量双重提升**：在目标检测和语义分割任务上，相比SOTA的帧级增强方法（如Nemo、NeuroScaler），**分析精度提升10-19%**，同时**端到端吞吐量提升2-3倍**。
- **资源利用效率极高**：相比全帧增强，可节省**高达77%的GPU计算资源**，将宝贵的算力集中用于对分析任务真正有益的区域。
- **强大的平台通用性**：在五种异构硬件平台（从云端A100到边缘Jetson）上均表现出**强大的鲁棒性和有效性**，证明了其在真实边缘计算场景中的实用价值。


## Accelerating Aggregation Queries on Unstructured Streams of Data
**本文由斯坦福大学、芝加哥大学、伊利诺伊大学厄巴纳-香槟分校合作完成，发表在会议 VLDB '23。项目主页见 https://github.com/stanford-futuredata/InQuest**

### 1. 动机：现有数据分析系统在处理非结构化数据流时面临根本性困境

- **问题核心**：现有数据分析系统在处理视频、文本等非结构化数据流的聚合查询时，存在**实时性、成本与准确性**的根本矛盾。对**数据流中的每一个元素**（如每一帧视频、每一条推文）都运行高精度的深度学习模型（**Oracle**）会产生巨大的计算开销和延迟，无法满足实时性要求；而简单的采样或使用低精度模型又无法保证查询结果的**准确性**。

- **现有方案的瓶颈**：
    - **批处理系统（Batch Systems）**：许多先进的查询加速系统（如 ABae）被设计为批处理模式，它们需要**预先访问整个数据集**来规划采样策略或训练专用模型，这在数据持续产生、无法预知未来的流式场景（Streaming Scenarios）中完全不适用。
    - **领域特定系统（Domain-Specific Systems）**：部分流式处理系统仅为特定数据模态（如仅视频）或特定模型架构设计，缺乏在多模态数据上进行通用查询的能力。

### 2. 方法：提出一个名为 InQuest 的系统，通过智能自适应的采样框架来解决上述困境

InQuest 将数据流切分为 **数据段**（Segments）进行处理，其核心是一个由三大创新组件构成的闭环自适应学习系统，旨在以最低的 **Oracle 调用成本** 实现最高的查询精度。

- **廉价代理引导的动态分层 (Proxy-Guided Dynamic Stratification)**：系统不直接处理原始数据，而是首先使用一个**超轻量级的代理模型（Proxy）**为数据流中的每个元素快速生成一个分数。InQuest 以此分数为基础，将数据流动态地划分为多个内部同质的**分层（Strata）**。该分层策略并非固定不变，而是通过分析上一个数据段的 Proxy 分数**分位数（Quantiles）**，并结合**指数加权移动平均（EWMA）**进行平滑，从而能够持续地**自适应**于数据分布的实时变化。

- **基于统计的自适应预算分配 (Statistics-driven Adaptive Budget Allocation)**：在分层的基础上，InQuest 智能地决定在每个分层中投入多少 **Oracle 调用预算**。该决策依据经典的**奈曼分配（Neyman Allocation）**理论，会优先将更多预算分配给：1) **不确定性更高**（即估计的**标准差 `σ̂`** 更大）的分层；2) **信息价值更高**（即估计的**谓词为正比例 `p̂`** 更高）的分层。同时，系统引入**防御性采样（Defensive Sampling）**机制，为每个分层预留一小部分固定预算，极大地增强了算法在面对突变时的**鲁棒性**。整个分配方案同样通过EWMA进行平滑，以实现稳健的持续学习。

- **保证流式无偏的采样执行 (Unbiased Streaming Sample Execution)**：在确定了各分层的采样数量后，由于无法预知每个分层在当前数据段的总大小，InQuest 为每个分层部署了一个独立的**水塘抽样（Reservoir Sampling）**。该技术能够在不知道数据流总长度的情况下，公平、无偏地从该分层的所有元素中抽取指定数量的样本。只有被水塘抽样选中的样本，才会“按需”触发昂贵的 Oracle 调用，从而将计算成本降至最低，并保证了最终统计估计的**无偏性**。

### 3. 效果：在查询精度、系统吞吐量和成本节约上均取得SOTA性能

- **成本与吞吐量巨幅优化**：在达到与流式基线方法（均匀采样、固定分层采样）相同的查询精度（RMSE）时，InQuest 所需的 **Oracle 调用次数（即成本）减少了高达 5.0 倍**，显著提升了系统的处理吞吐量。

- **查询精度显著提升**：在固定的 Oracle 调用预算下，相比为批处理场景设计的 SOTA 方法（如 ABae），InQuest 在流式场景下的**查询均方根误差（RMSE）降低了高达 1.9 倍**。这证明了其分段学习策略能有效利用数据的时序局部性，甚至超越了“总览全局”的批处理方法。

- **强大的鲁棒性与通用性**：
    - 通过**对抗性漂移测试**，证明了 InQuest 即使在数据分布频繁突变的非平稳流上依然能保持稳健性能，展现了其强大的自适应能力。
    - 在**视频和文本两种模态**的六个真实世界数据集上均验证了其有效性，证明了其框架的**通用性**。
    - **消融实验**证明了其动态分层和动态预算分配两大核心组件对于实现高性能均不可或缺。


## COVA: Exploiting Compressed-Domain Analysis to Accelerate Video Analytics
**本文由韩国科学技术院（KAIST）与谷歌（Google）合作完成，发表在会议 USENIX ATC '22。项目主页见 https://github.com/casys-kaist/CoVA**

### 1. 动机：现有视频分析系统在处理大规模视频数据时面临新的性能瓶颈

- **问题核心**：现代视频分析系统通过级联（Cascade）架构，成功缓解了昂贵的深度学习模型（DNN）的推理瓶颈。然而，它们普遍忽视或规避了一个更基础的瓶颈：**视频解码**。在对任何一帧进行分析前，必须先将压缩的视频流（如H.264格式）解码为原始像素数据，这一过程本身就非常缓慢，尤其是在处理高清视频时，解码速度甚至远低于后续的分析速度，成为了整个系统的**新瓶颈**。

- **现有方案的瓶颈**：
    - **高昂的预处理成本**：现有系统为了绕开解码瓶颈，通常采用两种昂贵的策略：1) **提前解码**，将整个视频库以原始像素格式存储，导致存储成本激增数十倍；2) **提前转码**，将视频在不同分辨率下预先存储多份，同样带来巨大的存储和计算开销。这两种方式在处理PB级视频数据时都不可行。
    - **有限的查询能力**：为了追求极致的吞吐量，许多级联系统被高度特化，仅支持简单的**时间查询**（例如，“视频中何时出现了汽车？”），而无法支持更具实用价值的**空间查询**（例如，“在画面左上角区域出现了几辆汽车？”），这极大地限制了系统的应用场景。

### 2. 方法：提出一个名为 COVA 的混合域级联分析框架，从根本上解决解码瓶颈

COVA 的核心思想是将分析任务**智能地拆分到两个域**中执行：在无需解码的**压缩域**完成大部分粗粒度的筛选和跟踪工作，仅对极少数关键帧在**像素域**进行昂贵的解码和精准分析。该框架由三大创新阶段构成，形成一个高效的分析流水线。

- **第一阶段：压缩域轨迹检测**：此阶段完全在压缩数据上操作，目标是快速定位并跟踪移动物体。
    - **轻量级移动物体检测 (`BlobNet`)**：COVA 设计了一个超轻量级的神经网络 **BlobNet**，它不读取任何像素信息，而是直接以视频的**压缩元数据**（如运动矢量、宏块类型和分区模式）作为输入，快速输出一个标记了移动物体（**Blobs**）大致位置的掩码。为了适应不同视频的特性（如镜头角度、光照），`BlobNet` 会针对每个视频进行一次性的、自动化的**即时特化训练**。
    - **高效物体跟踪**：将 `BlobNet` 在连续帧上检测到的“块（Blobs）”通过高效的跟踪算法（如SORT）连接起来，形成代表单个物体连续运动的**轨迹（Tracks）**。至此，系统已在不解码的情况下掌握了“何时、何处”有移动物体。

- **第二阶段：解码成本感知的锚点帧选择 (Decoding-Cost-Aware Anchor Frame Selection)**：此阶段的目标是从每个物体的运动轨迹中，智能地选择一个或少数几个最具代表性且**解码成本最低**的**锚帧（Anchor Frames）**。
    - **解码成本建模**：COVA利用了视频编码中**GOP**（Group of Pictures）的依赖结构。解码一个序列（GOP）中靠后的P帧或B帧，需要先解码它所依赖的所有前序帧。因此，一个帧在GOP中的位置越靠前，其解码成本越低。
    - **智能选择算法**：COVA 设计了一个贪心算法，它会为每个即将结束的运动轨迹，选择一个能够覆盖该物体、且在GOP中位置尽可能靠前（即依赖链最短）的帧作为锚点帧。这确保了所有物体都能被识别，同时将解码的总工作量降至最低。

- **第三阶段：像素域标签传播 (Pixel-Domain Label Propagation)**：这是唯一需要解码和运行高精度DNN的阶段。
    - **选择性解码与推理**：系统**仅解码**第二阶段选出的极少数锚点帧及其依赖帧，然后将这些解码后的帧送入一个高精度的DNN模型（如 YOLOv4）进行物体识别，获得准确的标签（如“汽车”）和边界框。
    - **标签关联与传播**：将DNN在锚点帧上识别出的精确物体，与第一阶段在压缩域中检测到的轨迹进行空间位置上的关联。一旦关联成功，该物体的标签就会被**传播**到其对应轨迹所覆盖的**所有帧**上。这样，COVA仅通过分析几帧就完成了对整个物体运动片段的标注。

### 3. 效果：在系统吞吐量上取得SOTA性能，同时保证了可接受的精度并扩展了查询能力

- **系统吞吐量巨幅提升**：与被解码瓶颈限制的传统级联系统相比，COVA 的端到端处理吞吐量**平均提升了 4.8 倍**。这是因为它成功地将超过 **73%** 的帧从昂贵的解码和DNN推理流水线中过滤掉，从根本上解决了解码瓶颈。

- **可接受的精度与成本权衡**：这种巨大的性能提升带来了约10-20%的“适度”准确率损失。论文论证，对于旨在快速从海量视频中进行探索性分析和获取高层洞察的应用场景，这种权衡是完全可以接受且极具价值的。因为对于回放性视频分析，提供的往往只是近似结果。

- **强大的通用性与扩展性**：
    - **原生支持空间查询**：由于COVA的框架本身就处理和保留了物体的时空轨迹信息，因此它能够**原生支持空间查询**，且与时间查询相比没有额外的性能或精度损失，显著优于现有系统。
    - **多编码器兼容**：通过敏感性分析，证明了COVA的设计理念不局限于H.264，同样适用于VP8、VP9、HEVC等其他主流的基于块的视频编码标准，展现了其框架的**通用性**。
    - **瓶颈分析**：消融实验证明，COVA的流水线设计是有效的，其新的性能瓶颈取决于视频内容，可能落在解码阶段（对于运动密集的视频）或DNN推理阶段（对于运动稀疏的视频），但整体吞吐量远高于基线系统。


## Gemel: Model Merging for Memory-Efficient, Real-Time Video Analytics at the Edge
**本文由UCLA、普林斯顿大学、微软研究院、浙江大学合作完成，发表在会议 NSDI '23。项目主页见 https://github.com/artpad6/gemel_nsdi23**

### 1. 动机：现有边缘视频分析系统在处理多模型实时任务时面临根本性的显存困境

**核心问题：** 随着摄像头和AI应用的普及，边缘计算节点（Edge Box）需要同时运行越来越多、越来越复杂的深度学习模型来进行实时视频分析。然而，这些边缘设备的GPU显存资源极其有限且增长缓慢，导致在满足**实时性、低成本和高准确性**三者之间存在根本性的矛盾。当多个模型的总大小超过GPU显存时，系统不得不频繁地在CPU和GPU之间交换模型，这一过程的巨大延迟开销导致了严重的性能瓶颈。

**现有方案的瓶颈：**

*   **时间/空间共享 (Time/Space Sharing)**：这是最直接的应对策略，即通过分时复用GPU来运行超出显存容量的模型。但其致命缺陷在于**模型加载开销巨大**。论文通过实验（Table 1）证明，将一个模型从CPU加载到GPU的时间（几十到上百毫秒）往往远超其单次推理时间（几毫秒），有时甚至高达**34倍**。对于要求每帧处理时间在33毫秒以内的实时视频流，这种延迟是不可接受的，直接导致系统**大量丢帧（高达19-84%）**，最终造成**端到端任务准确率雪崩（下降高达43%）**。
*   **模型压缩/量化 (Compression/Quantization)**：这些技术通过生成更轻量的模型来降低显存和计算开销。然而，它们通常以牺牲模型准确率为代价，且对漂移（drift）更敏感。更重要的是，确定哪种压缩程度对特定任务和部署环境是合适的，需要大量领域专家的手动调优，缺乏通用性和自动化。此外，即便压缩后，多个模型依然可能超出显存限制。
*   **主干共享 (Stem-Sharing)**：一些多任务学习系统（如Mainstream）通过共享模型**起始部分连续的层（即“主干网络”）**来节省计算。但这种方法有两个局限：1) 它要求共享的层必须是连续的；2) 视觉模型中**最占用显存的“重量级”层**（如全连接层）通常位于模型的**末端**，主干共享无法触及这些层，因此内存节省效果有限。

### 2. 方法：提出一个名为 Gemel 的系统，通过智能的“模型合并”框架来解决上述困境

Gemel的核心思想是，利用不同视觉模型间广泛存在的**架构共性**，让多个模型**共享**其内部任意位置的、架构相同的层（包括权重），从而显著降低整个工作负载的显存占用。这是一个典型的云边协同系统，将复杂的合并与训练过程置于云端，而将高效的推理执行置于边缘。

**1. 指导性观察：模型合并的理论基石**
Gemel的设计基于两大关键的经验性观察，它们将一个复杂的组合优化问题极大地简化：

*   **幂律内存分布**：在一个DNN模型中，绝大多数显存（60-91%）被极少数（约15%）的“重量级”层占据。**启发**：无需合并所有层，只需优先合并这几个“重量级”层，就能以最小的准确率风险，获得最大的显存收益。
*   **每层合并决策的独立性**：一个层能否成功合并（即重训练后准确率达标），基本上**不依赖于**其他层是否也被合并。**启发**：可以将全局优化问题分解为一系列独立的、可增量解决的决策问题，极大地降低了搜索空间的复杂度。

**2. 智能合并启发式算法**
Gemel在云端运行一套高效的启发式算法，以自动发现最佳的合并方案：

*   **收益驱动的增量式合并**：系统首先识别所有可共享的层，并根据它们能带来的**潜在内存节省量**进行降序排序。然后，Gemel**一次只尝试合并一个（或一组）收益最高的层**，并在重训练成功后，保留该合并配置，再继续尝试下一个，从而逐步、稳健地构建最终的合并方案。
*   **自适应重训练**：为了将昂贵的“试错成本”降至最低，重训练过程是自适应的。它能**提前检测成功**（在准确率接近目标时，减少训练数据以加速收敛）和**提前检测失败**（在准确率迟迟不提升时，提前中止训练），显著减少了不必要的时间开销（平均减少28%的重训练时间）。
*   **鲁棒的失败处理**：当一次合并尝试失败时，系统不会简单放弃，而是会尝试一个更保守的策略（如缩小共享范围），在“收益”和“成功率”之间做出智能权衡。

**3. 合并感知的边缘推理调度**
一旦云端找到了成功的合并方案，更新后的共享权重会被部署到边缘。边缘的推理调度器也变得更加智能：

*   它会识别出哪些模型之间共享了层，并倾向于**将共享度高的模型安排在一起连续执行**。
*   当切换模型时，它**只加载那些非共享的、独有的层**，从而将模型交换的延迟降至最低，最大化模型合并带来的性能收益。

### 3. 效果：在准确率、显存效率和系统性能上均取得SOTA表现

**端到端准确率巨幅提升**：这是Gemel最核心的贡献。与仅使用传统时间/空间共享策略的基线相比，Gemel通过减少丢帧（多处理13-44%的帧），将端到端的任务准确率**提升了8-39%**。

**显存效率显著优化**：
*   Gemel成功将工作负载的整体显存需求**降低了17.5-60.7%**，换算成绝对值高达**0.2-5.1 GB**。
*   与只能共享模型头部的“主干共享”方法（Mainstream）相比，Gemel的内存节省效果要**高出5.9-52.3%**，因为它能合并模型任意位置的、更占内存的层。
*   Gemel的合并结果非常接近理论最优值（在9.3-29.0%的差距内），展示了其启发式算法的高效性。

**强大的性能与效率**：
*   Gemel的启发式算法非常高效，**超过70%的内存节省是在最初的24-210分钟内实现的**，证明其“优先合并重量级层”的策略极为有效。
*   该系统是通用的，其效果在覆盖多种模型架构、任务类型和视频场景的**超过850个**多样化工作负载上得到了验证，展现了其强大的通用性和鲁棒性。
*   消融实验证明，其增量式合并和自适应重训练等关键设计对于实现高性能至关重要。


## Extract-Transform-Load for Video Streams
**本文由MIT CSAIL、亚利桑那大学、AWS、Intel Labs合作完成，发表在会议 PVLDB '23。项目主页见 https://github.com/ferdiko/vetl**

### 1. 动机：现有大规模视频流分析面临的困境

**问题核心：** 随着城市监控、自动驾驶等场景产生海量的视频数据流，如何经济高效地从中提取有价值的信息成为一个巨大挑战。这其中存在一个难题：**低成本、高吞吐（实时性）和高精度**难以兼得。
*   **高成本**：存储原始视频流的开销惊人（千台摄像头每月230TB，存储费6万美元/年）；在查询时对全部数据运行昂贵的CV模型更是成本高昂（百台摄像头一个月的分析成本超10万美元）。
*   **低吞-吐/高延迟**：即使在现代GPU上，先进的CV模型每秒也只能处理寥寥数帧。分析长达一年的视频数据可能需要数月时间，无法满足实时或近实时的决策需求。
*   **精度损失**：为了降低成本和延迟，最直接的方法是降低采样率或使用低质量配置，但这会严重损害分析结果的准确性。

**现有方案的瓶颈：**
*   **朴素方法**：先存储所有视频，再在查询时处理。这种方法在存储和计算上都**成本过高**，且查询**延迟巨大**，完全不实用。
*   **传统数据仓库范式**：作者提出，视频分析可以类比为数据仓库问题。视频流是“易于生产但难以查询”的原始数据，需要一个ETL过程将其转化为“易于查询”的结构化数据（如检测结果表）。作者将此过程定义为 **Video Extract-Transform-Load(V-ETL)**。然而，这个V-ETL的`Transform`步骤（即运行CV模型）本身就是成本和性能的瓶颈所在。
*   **现有内容自适应系统**：这些系统能根据视频内容的难易程度动态调整处理配置（如帧率、分辨率），在内容简单时使用廉价配置，在内容复杂时使用昂贵配置。但它们的核心假设是**计算资源是峰值配置的**，即硬件能力足以应对最坏情况下的工作负载。这在追求低成本的V-ETL场景下不成立，这些系统在资源受限时**无法保证吞吐量**，会导致数据积压和系统崩溃。

### 2. 方法：提出一个名为 Skyscraper 的系统，通过预测性规划与反应式执行结合的框架，实现低成本、高吞吐的V-ETL

Skyscraper旨在**预算受限**的硬件上，通过智能地**自适应调优**CV工作流的“旋钮(knobs)”（如帧率、模型精度、图像分块等），在保证数据流不积压（吞吐量）的前提下，**最大化分析结果的质量**。这是一个结合了离线学习和在线决策的复杂系统。

**1. 核心理念：解耦预测与执行，结合长期规划与短期反应**
Skyscraper认识到，精确预测未来某一时刻视频的具体内容（如“3小时后下午2:15:30会有一群人经过”）是不可能的。但预测**未来一段时间内不同类型内容的分布**（如“未来一天中，有10%的时间是交通高峰期”）是可行的。

*   **离线学习阶段 (Offline Learning Phase)**：
    *   **内容分类**：系统利用少量无标签数据，通过K-Means聚类，自动将视频流划分为几个具有代表性的“内容类别”（如：交通稀疏、交通正常、交通拥堵）。分类的依据是不同“旋钮”配置在这些内容上产生的质量向量，确保同一类别内的内容对于所有配置都表现出相似的质量。
    *   **模型训练**：训练一个简单的前馈神经网络来**预测在未来一个规划周期（如两天）内，每种内容类别出现的频率**。
*   **在线摄取阶段 (Online Ingestion Phase)**：
    *   **预测性旋钮规划器 (Predictive Knob Planner)**：该组件**定期运行**（如每两天一次）。它利用预测模型得到的未来内容分布频率，通过一个**线性规划**求解器，为每个内容类别**静态地分配一个最优的“旋钮”配置组合**。目标是在满足总预算和吞吐量约束下，最大化期望的全局质量。这本质上是一个**资源分配**问题。
    *   **反应式旋钮切换器 (Reactive Knob Switcher)**：该组件**高频运行**（如每两秒一次）。它负责**实时地**：1) 快速判断当前流入的视频帧属于哪个内容类别（通过比较当前配置产生的质量与各类别中心的距离）；2) 查找规划器为该类别分配好的旋钮配置；3) 结合当前**缓冲区使用情况和云端资源**，做出最终的执行决策，以确保系统不掉队。

**2. 资源管理：本地计算、缓冲区与云爆发的协同**
Skyscraper明确地管理三种资源，以应对工作负载的波动：
*   **本地计算集群 (Local Compute Cluster)**：用户根据预算配置的廉价、永远在线的计算资源，是处理视频流的主力。
*   **视频缓冲区 (Video Buffer)**：当本地资源无法实时处理（如遇到复杂内容需要昂贵配置时），视频帧会被暂时存入缓冲区，等待后续处理。
*   **云爆发 (Cloud Bursting)**：当缓冲区即将溢出时，系统会将部分工作负载“爆发”到云端进行处理，以保证吞吐量。Skyscraper的规划器会智能地预算和使用云端额度，避免过早耗尽。

### 3. 效果：在成本节约、质量和鲁棒性上均取得SOTA性能

**成本-质量权衡巨幅优化**：
*   与采用静态最优配置的基线相比，Skyscraper在各种真实世界工作负载（交通监控、疫情安全检测、社交媒体情感分析）上均展现了**显著更优的成本-质量曲线**。
*   在达到与静态基线和Chameleon（一个带缓冲区的适配版）相同的质量水平时，Skyscraper的**成本降低了高达8.7倍**。

**强大的资源利用与鲁棒性**：
*   通过消融实验证明，**缓冲区和云爆发的结合**是实现最佳性能的关键。在面对短时高峰时，带宽限制使得云爆发效果不佳，此时缓冲区起主导作用；在面对长时高峰时，缓冲区会很快耗尽，此时云爆发成为关键。Skyscraper能智能协同两者，应对不同模式的负载尖峰。
*   其**预测性规划器**和**反应式切换器**的开销极低，对在线性能影响可忽略不计。
*   预测模型的准确性很高，且系统对预测误差不敏感。即使在长达8天的预测窗口下性能有所下降，其结果依然远超基线，证明了其框架的**鲁棒性**。

**通用性与实用性**：
*   Skyscraper的框架是**通用**的，不依赖于特定的CV模型或任务。用户只需定义工作流的DAG、可调的“旋钮”以及一个返回**质量指标**的函数即可。
*   它解决了V-ETL这一类重要但之前未被明确定义的问题，为大规模、低成本视频数据仓库的构建提供了第一个实用且高效的解决方案。

## GRACE: Loss-Resilient Real-Time Video through Neural Codecs
**本文由芝加哥大学、斯坦福大学、微软、NVIDIA合作完成，发表在会议 USENIX NSDI '24。项目主页见 https://uchi-jcl.github.io/grace.html**

### 1. 动机：现有实时视频抗丢包技术面临的困境

**问题核心：** 在实时视频通信（如视频会议、云游戏）中，由于对延迟的极高要求，无法通过重传恢复丢失的数据包。现有技术在应对网络丢包时，难以在**低延迟、高画质和高流畅度**之间取得理想的平衡。
*   **高延迟/卡顿**：为了保证数据完整性，传统方法或等待重传，或在丢包后请求全新的关键帧，这些都会导致明显的视频卡顿和延迟。
*   **画质下降**：为了降低丢包影响，一些方法以牺牲画质为代价。例如，增加过多的冗余（FEC）会占用带宽，导致基础画质变差；将一帧切成独立小块以进行错误隐藏，会降低压缩效率。
*   **恢复效果差**：当丢包发生时，传统的错误隐藏技术由于缺乏编码器的“指导”，只能进行“盲猜”，恢复出的画面往往充满马赛克和伪影，体验不佳。

**现有方案的瓶颈：**
*   **前向纠错 (Forward Error Correction, FEC)**：这种方法在发送端添加冗余包。其核心缺陷是**需要精确预测未来的网络丢包率**。如果冗余加少了，丢包一多，视频流就会直接崩溃；如果冗余加多了，会持续浪费大量带宽，导致整体画质下降。
*   **错误隐藏 (Error Concealment, EC)**：这种方法在接收端尝试“猜测”和填补丢失的画面部分。其核心缺陷是**编解码器解耦**。编码器为了追求极致压缩，去除了所有冗余，没有为解码器的“猜测”提供任何额外信息。这导致解码器只能基于有限的上下文进行恢复，效果很差。
*   **分离设计范式的局限**：现有系统普遍遵循经典的“信源-信道分离”设计原则，即“压缩”和“抗丢包”是两个独立的、串联的模块。这种分离设计导致了上述问题，因为压缩过程丢弃了对恢复有用的信息，而抗丢包过程又不了解视频内容的具体重要性。

### 2. 方法：提出一个名为 GRACE 的系统，通过端到端联合训练的神经网络编解码器，将抗丢包能力内化到编解码器自身

GRACE 的核心思想是**打破“压缩”和“抗丢包”的分离设计**，通过一种新颖的训练范式，让视频编解码器本身就“学会”如何在不完美的网络中进行鲁棒的通信。这是一个算法与系统深度结合的解决方案。

**1. 核心理念：通过模拟丢包进行端到端的联合训练**
GRACE 的基石是**神经网络视频编解码器 (NVC)**。其强大的抗丢包能力来源于一个创新的训练过程：
*   **训练时模拟丢包 (Simulated Packet Loss)**：在训练流程中，GRACE 在编码器输出特征张量后，会人为地、**随机地将其中一部分元素置为零（随机掩码）**，然后再送入解码器进行重建。
*   **联合优化 (Joint Optimization)**：这个“模拟丢包”操作强迫编码器和解码器进行协同进化：
    *   **编码器学会“智能冗余”**：为了让解码器在信息不全时仍能恢复图像，编码器必须学会将一个重要的视觉信息**分散地、冗余地**编码到特征张量的多个位置。
    *   **解码器学会“鲁棒重建”**：解码器在训练中见过了成千上万种“残缺”的输入，学会了如何从不完整的线索中**推理和重建**出最可能的原始图像。

**2. 关键系统设计：保障鲁棒性发挥的工程实现**
为了将训练好的模型落地为可用的实时系统，GRACE 设计了多个关键机制：
*   **可逆随机分包 (Reversible Randomized Packetization)**：设计了一种巧妙的分包策略，它将编码后的特征张量元素随机打乱再分装进网络包。这使得真实世界中**“丢掉一个数据包”**的效果，与训练时**“随机置零一部分元素”**的效果完美等价，确保了模型的学以致用。
*   **乐观编码与动态状态重同步 (Optimistic Encoding & Dynamic State Resync)**：为了解决丢包导致的编解码器参考帧不一致问题，GRACE 提出了一种创新的协议：
    *   **乐观编码**：编码器总是假设网络良好，持续编码下一帧，保证了视频流的**流畅性**。
    *   **动态重同步**：当解码器检测到丢包并使用了不完整的参考帧后，会立刻通过一个**轻量级的反馈**通知编码器。编码器在本地**极速地**重新计算出与解码器同步的参考帧，从而在不中断视频流的情况下，快速消除错误传播。
*   **效率与码率控制 (GRACE-Lite & Bitrate Control)**：为了在普通 CPU 和移动设备上实现实时运行，提出了轻量化的 **GRACE-Lite** 版本。同时，通过重用运动估计结果和预训练多个残差编码器，实现了对输出码率的快速、精确控制。

### 3. 效果：在流畅度、画质和用户体验上均取得SOTA性能

**流畅度与可靠性巨幅提升**：
*   与最先进的 FEC 方案 (Tambur) 相比，在真实网络轨迹下，GRACE **将无法解码的帧数减少了 95%，将视频卡顿时长减少了 90%**。
*   其创新的状态同步协议避免了传统方法因等待重传或请求I帧而造成的巨大延迟。

**有丢包情况下的画质优势**：
*   GRACE 的画质随丢包率增加而**“优雅地”下降**，而不是像 FEC 那样出现“悬崖式”崩溃。
*   与基于神经网络的错误隐藏方案相比，GRACE 在 20%-80% 的丢包率下，视频质量指标 (SSIM) **高出 0.5-4 dB**。
*   在包含 240 名参与者和 960 次评分的用户研究中，GRACE 的**平均主观意见分 (MOS) 比所有基准方案高出 38%**。

**无丢包情况下的性能对标**：
*   在网络状况良好时，GRACE 的压缩效率与 H.265 等主流传统编解码器**相当**。
*   其**GRACE-Lite**版本在 iPhone 14 Pro 等移动设备上能达到超过 25fps 的实时编码速度，证明了其框架的**实用性**。

**通用性与开创性**：
*   GRACE 的框架是**通用**的，可以与任何拥塞控制算法集成。
*   它**首次**系统性地提出并实现了一种全新的抗丢包范式，即通过联合训练将网络适应性内化到编解码器中，为解决实时通信中的长期痛点问题提供了第一个高效且优雅的解决方案。


## Edge-assisted Adaptive Configuration for Serverless-based Video Analytics
**本文由清华大学与华为合作完成，发表在会议 IEEE ICDCS '23，代码见：https://github.com/STAR-Tsinghua/ServerlessVideoAnalytics。**

### 1. 动机：现有视频分析系统面临成本与准确率的两难困境

**问题核心：** 现代视频分析严重依赖于计算密集型的深度神经网络，导致运行成本高昂。传统的资源配置方式难以在**低成本、高准确率和动态适应性**之间取得理想的平衡。
*   **成本高昂/资源浪费**：传统方案通常采用“峰值配置”策略，即为云端服务器或边缘设备配置足以处理最复杂视频场景的资源。然而，视频内容的复杂性是动态变化的，在大部分时间里，这些资源处于闲置状态，造成了巨大的成本浪费。
*   **精度无法保证**：如果为了省钱而采用低配方案（如使用小模型、低帧率），当视频内容变得复杂时（如物体快速移动），分析准确率会急剧下降，无法满足应用需求。
*   **配置僵化**：现有的视频分析系统通常采用一套固定的配置（模型、帧率、计算资源）来处理所有视频流，无法根据实时变化的视频内容和网络状况进行动态调整，缺乏灵活性和效率。

**现有方案的瓶颈：**
*   **云端集中处理**：将所有原始视频流发送到云端进行分析，虽然算力强大，但网络传输开销大，且对所有帧进行昂贵的DNN推理，成本极高。
*   **边缘独立处理**：完全在边缘设备上运行分析，受限于边缘设备有限的计算和存储能力，难以部署高精度的大型DNN模型。
*   **分离的优化目标**：现有研究通常只关注优化其中一个或两个配置“旋钮”（如只调整帧率或模型），而忽略了视频配置（模型、帧率）和平台配置（计算资源）之间的复杂相互作用。这种分离的优化方式无法找到全局最优的成本-准确率平衡点。

### 2. 方法：提出一个边缘辅助的无服务器视频分析框架，通过算法动态联合调整配置，实现成本最小化

该框架的核心思想是**将边缘的轻量级追踪与云端的重量级检测相结合，并利用一个智能算法来动态指挥整个系统**，从而在满足精度要求的前提下，最大限度地降低由无服务器平台产生的计算成本。

**1. 核心理念：边缘-云协同与动态配置**
框架通过将任务智能地分配给边缘和云端，并动态调整三类关键“旋钮”来实现优化：
*   **分工合作**：
    *   **边缘服务器**：负责轻量级任务，包括视频解码、**目标跟踪**，以及运行核心的**自适应配置算法**。
    *   **云端无服务器平台 (Serverless Platform)**：只在被选取的关键帧上执行昂贵的 **DNN 对象检测**。
*   **动态联合优化**：算法的核心是联合调整三个关键配置：
    *   **DNN 模型**：在多个不同大小和精度的模型（如 YOLOv5x, s, n）之间选择。
    *   **帧率**：决定发送多少关键帧到云端进行检测。
    *   **计算资源**：在无服务器平台（如AWS Lambda）上为函数选择最优的内存大小，以间接控制CPU算力和成本。

**2. 关键系统设计与算法：保障成本-精度最优**
为了将这一理念落地，系统设计了清晰的架构和高效的算法：
*   **系统架构**：设计了一个清晰的**数据流**（视频帧流动）和**控制流**（配置决策流动）分离的边缘-云协同架构。边缘服务器作为“大脑”，指挥云端无服务器函数的工作模式。
*   **问题建模**：将问题形式化为一个**带约束的非线性优化问题**，目标是在满足**最低准确率要求 (`A`)** 的约束下，**最小化计算成本 (`Fc`)**。
*   **自适应配置算法**：由于问题是NP-hard的，作者提出了一种基于**马尔可夫近似** 的高效启发式算法。该算法通过迭代搜索，智能地探索（模型、帧率、内存）的组合空间，快速收敛到近似最优解。它能够根据视频内容变化，在每个决策周期输出一组新的最优配置。

### 3. 效果：在保证精度的同时，计算成本显著降低

**成本与精度优势**：
*   与两种固定配置的基线方案相比，在满足相同目标准确率的前提下，该自适应框架**将计算成本降低了高达 77%**。
*   与另一个先进的自适应视频分析系统 **Glimpse** 相比，该框架在成本上**降低了 9% 到 26%**，并且精度表现更稳定。

**动态适应性验证**：
*   实验清晰地展示了框架的自适应行为：当视频中物体运动速度加快时，算法会自动**提高发送到云端的帧率**以保证准确率；当画面静止时，则**降低帧率**以节省成本。
*   尽管配置（帧率、成本）在动态变化，最终的分析精度始终被**稳定地控制在用户设定的目标值附近**。

### PacketGame: Multi-Stream Packet Gating for Concurrent Video Inference at Scale
**本文由中国科学技术大学完成，发表在会议 ACM SIGCOMM '23，代码见：https://github.com/yuanmu97/PacketGame。**

### 1. 动机：发现被忽视的解码瓶颈，限制大规模并发能力

**问题核心：** 在大规模视频分析系统中，尽管推理模型和帧过滤等环节已被高度优化，但一个被长期忽视的环节——**视频解码**——成为了整个系统的端到端并发性能瓶颈。
*   **并发能力受限/解码成为瓶颈**：作者在运营一个超千路摄像头的真实系统中发现，经过优化的推理模块能支持高达3015路并发，而解码模块（即便使用12核CPU）却只能支持35路。原因是解码器必须处理每一路视频流的所有数据包，而后续的推理模型仅需处理被过滤后不到2%的关键帧。这导致了严重的“头重脚轻”问题。
*   **现有优化方案的局限性**：
    *   **服务器端帧过滤 (On-server Filtering)**：发生在解码之后，无法减轻解码器的负担，治标不治本。
    *   **端侧过滤/专用压缩 (On-camera Filtering/Specialized Compression)**：需要修改摄像头固件或使用特定的视频编码器，缺乏对现有商用设备和离线视频的通用性。
    *   **模型加速 (Model Acceleration)**：只优化了流水线的末端，反而加剧了解码环节的瓶颈效应。
*   **协调机制缺失**：在处理上百路并发视频流时，如何将有限的解码资源动态分配给“最有价值”的视频流是一个巨大挑战。简单的轮询调度策略效率极低，与理想的全局调度相比，并发能力相差近两个数量级。

### 2. 方法：在解码前引入“数据包门控”框架 (PacketGame)，智能过滤数据包

该框架的核心思想是在视频被解码之前，引入一个全新的“数据包门控”阶段。它通过一个轻量级、有理论保证的算法，仅根据编码后的数据包元数据（如包大小、帧类型）来智能判断是否值得解码，从而直接从根源上解决解码瓶颈。

**1. 核心理念：解码前过滤与全局协调**
*   **攻击瓶颈**：将过滤决策前移至解码器之前。这不仅减少了解码的计算开销，也自然地减少了后续所有阶段的负载。
*   **通用设计**：该方法无需修改摄像头或视频源，作为一个纯软件插件即可部署，完美兼容现有的商用摄像头和视频文件。
*   **在线决策与优化**：将问题建模为一个在线学习问题，目标是在满足固定的解码预算下，最大化解码的“有效”数据包数量，从而保证高推理精度。

**2. 关键系统设计与算法：PacketGame 的三大模块**
为了实现这一理念，PacketGame 设计了一个由三个模块构成的闭环决策系统：
*   **上下文预测器 (Contextual Predictor)**：这是框架的“大脑”，一个超轻量级神经网络。它创新性地采用**多视角学习 (Multi-view Learning)**，将独立帧（I-frame）和预测帧（P/B-frame）的数据包大小序列作为不同视角输入，以捕捉场景复杂度与场景变化的双重信息，从而生成对数据包“必要性”的精准预测。
*   **时间估计器 (Temporal Estimator)**：作为“记忆”，它利用下游推理结果的在线反馈，采用类似**UCB (Upper Confidence Bound)** 的机制来平衡**探索与利用 (Exploration-Exploitation)**。它鼓励系统关注近期回报高的视频流（利用），同时也不会忽视长期未被检查的流（探索），以防错过突发事件。
*   **组合优化器 (Combinatorial Optimizer)**：作为“指挥官”，它负责全局资源分配。它将问题转化为一个近似背包问题，并采用一个高效的**贪心算法**，根据每个数据包的“置信度/解码成本”性价比进行排序，在总解码预算内做出全局最优的选择。

### 3. 效果：解码开销大幅降低，端到端并发能力显著提升

**解码效率与并发增益**：
*   与原始工作负载相比，在保证90%以上推理精度的前提下，PacketGame 能够节省 **52.0% 到 79.3%** 的解码计算成本。
*   在相同的解码资源下，PacketGame 实现了 **2.1 到 4.8 倍** 的端到端并发能力提升。

**与现有技术的互补性与优越性**：
*   实验中最具说服力的对比显示，在一个已经使用 TensorRT 进行模型加速的系统上（并发能力为30路），再集成服务器端帧过滤技术 InFi，并发能力仅提升至35路（瓶颈仍在解码）。
*   而将 InFi 替换为 PacketGame 后（`TRT+PacketGame`），系统的并发能力**上升至 169 路**，提升了近5倍。这证明了PacketGame在解决并发瓶颈上的独特价值和与现有技术的强大互补性。

### InFi: End-to-end Learnable Input Filter for Resource-efficient Mobile-centric Inference
**本文由中国科学技术大学完成，发表在会议 ACM MobiCom '22，代码见：https://github.com/yuanmu97/infi。**

### 1. 动机：现有输入过滤器设计缺乏理论指导且适用性有限

**问题核心：** 在移动端AI推理中，大量输入数据是冗余的（如不含人脸的图片、结果无变化的视频帧），直接对这些数据进行昂贵的推理计算造成了巨大的资源浪费。然而，现有的输入过滤方法存在三大瓶颈。
*   **缺乏理论指导**：设计过滤器更像是一门“手艺”，依赖于反复试验。开发者无法预先判断一个AI任务是否“值得”被过滤，导致开发成本高且效果无法保证。
*   **特征表示能力不足**：现有方法大多依赖手工设计的低级特征（如像素差、边缘信息）或通用的预训练模型特征。这些特征并非为“过滤”这个特定任务而优化，导致在多样化的AI任务中判别能力不足，过滤精度和效率低下。
*   **适用场景局限**：大多数过滤器仅为特定数据类型（如图像）或特定部署模式（如云端卸载）设计，无法处理移动应用中常见的文本、传感器信号等数据，也无法支持模型分区（Model Partitioning）等更先进的部署模式。

### 2. 方法：提出首个端到端可学习的通用输入过滤框架 (InFi)

该框架的核心思想是，将输入过滤视为一个可学习的、端到端优化的任务。它不仅为“何时该过滤”提供了理论依据，还通过一个统一的框架解决了“如何高效过滤”的问题，极大地扩展了过滤技术的适用范围。

**1. 核心理念：理论指导与端到端学习**
*   **提出“可过滤性”理论 (Filterability)**：首次从计算学习理论出发，通过比较推理模型与其过滤器的“假设族复杂度”，为判断一个AI任务是否适合被过滤提供了理论指导，避免了盲目开发。
*   **端到端可学习设计**：抛弃手工或固定特征，InFi的整个过滤模型（从特征提取到决策）都是为了“过滤”这个最终目标而协同优化的。这使得它能为每个具体任务学习到最具区分度的特征表示。
*   **统一SKIP与REUSE**：巧妙地将“跳过型”过滤（SKIP）和“重用型”过滤（REUSE）统一到一个基于孪生网络（Siamese Network）思想的框架下，用一套模型解决了两种主流的过滤需求。

**2. 关键系统设计与算法：InFi 的三大模块**
*   **多模态特征网络 (Multi-modal Feature Networks)**：作为框架的“感知层”，InFi设计了一系列轻量级的特征提取器，使其成为首个原生支持**图像、视频、音频、文本、传感器信号**以及**模型中间层特征图**六种模态的过滤框架，极大地增强了通用性。
*   **任务无关的分类器 (Task-agnostic Classifier)**：作为“决策核心”，它接收特征网络提取的嵌入向量，并作为一个**可学习的距离度量器**，计算出输入的冗余度分数。其结构通用，但参数会根据具体任务自适应优化。
*   **双模式端到端训练 (Dual-mode End-to-end Training)**：作为“学习引擎”，InFi根据过滤模式采用不同的训练策略：
    *   **SKIP模式**：视为一个**二元分类**问题，使用**交叉熵损失**进行训练。
    *   **REUSE模式**：视为一个**度量学习**问题，使用对比损失（Contrastive Loss）进行训练，学习一个高效的度量空间。

### 3. 效果：过滤效率和适用性全面领先，显著提升系统性能

**过滤效率与性能增益**：
*   与原始工作负载相比，在保证90%以上推理精度的前提下，InFi可为视频分析应用带来高达 **8.5倍** 的吞吐量提升，并节省 **95%** 的网络带宽。
*   其自身的计算和能耗开销极低，在移动设备上的推理延迟仅为标准轻量级模型MobileNetV1的 **12-25%**。

**与现有技术的互补性与优越性**：
*   在与多种基线方法（FilterForward, Reducto, FoggyCache）的对比中，InFi在所有12个测试任务上均取得了显著更优的**准确率-过滤率**权衡曲线。
*   其优越性根源在于**端到端学习到的定制化特征**。在许多基线方法因特征不适用而完全失效的任务上（如人脸检测、文本情感分析），InFi依然能保持高效过滤。
*   得益于其通用设计，InFi是首个能成功应用于**模型分区**部署模式的输入过滤器，展现了其在支持未来移动AI架构上的独特价值。


### MadEye: Boosting Live Video Analytics Accuracy with Adaptive Camera Configurations
**本文由普林斯顿大学、莱斯大学完成，发表在会议 USENIX NSDI '24，代码见：https://github.com/michaeldwong/madeye。**

### 1. 动机：现有视频分析系统忽视了摄像头方向这一关键因素

**问题核心：** 现有视频分析系统将摄像头视为一个被动的、固定的传感器，所有优化都集中在如何更高效地处理**已被捕获**的视频数据。然而，摄像头捕捉到的内容本身（即其方向、焦距）对最终分析的准确率有着决定性的影响，这一巨大的优化潜力被完全忽视了。主动调整摄像头方向面临三大瓶颈。
*   **变化快难预测**：最佳的摄像头方向（能带来最高准确率的视角）变化极快，85%的变化在1秒内就会发生。依赖历史数据进行预测的传统方法（如多臂老虎机）因其滞后性而完全失效。
*   **需求多样难统一**：不同的AI任务（如检测车辆 vs. 计数行人）、不同的模型、不同的物体，对“最佳方向”的定义天差地别。一个固定的方向或一种简单的调整策略，无法同时满足复杂工作负载下的多样化需求。
*   **搜索空间巨大**：摄像头所有可能的方向（旋转、俯仰、变焦）构成了一个巨大的搜索空间，而其中的“最优解”既稀疏又转瞬即逝。盲目或暴力搜索在实时应用中是完全不可行的。

### 2. 方法：提出首个通过自适应摄像头配置提升分析精度的协同框架 (MadEye)

该框架的核心思想是，将摄像头从被动的传感器转变为主动的“智能体”，通过一个在摄像头和服务器之间高效协同的系统，实时地探索并选择能最大化整个工作负载分析精度的视角。

**1. 核心理念：经验观察指导与“近似模型+高精度模型”协同架构**
*   **经验观察指导**：系统设计并非盲目进行，而是基于对视频数据的三个关键经验性观察：（1）最佳方向在空间上是缓慢移动的；（2）高价值方向在空间上是聚集的；（3）相邻方向的准确率变化趋势是高度相关的。这些观察为设计高效的本地搜索算法提供了理论依据。
*   **端到端协同优化**：MadEye是一个完整的端到端系统。服务器负责高精度推理和模型训练，而摄像头负责快速探索和初步筛选，二者紧密协作，共同优化“从捕捉到分析”的整个流程。
*   **“近似模型+高精度模型”架构**：巧妙地将系统分工。摄像头上运行超轻量级的**近似模型**，其唯一任务是在极短时间内快速评估大量潜在方向的价值；服务器则保留**高精度模型**，只对近似模型筛选出的最有价值的图像进行精确打击，从而实现资源效率和准确率的平衡。

**2. 关键系统设计与算法：MadEye 的三大模块**
*   **近似模型与知识蒸馏 (Approximation Model & Knowledge Distillation)**：作为框架的“感知层”，MadEye不为每个任务设计专用模型，而是统一采用一个轻量级**物体检测器**作为通用近似模型。该模型通过**知识蒸馏**从服务器端的重型模型中学习其“偏好”，并能通过**持续学习**不断自适应场景变化。
*   **快速本地搜索算法 (Fast Local Search Algorithm)**：作为“决策核心”，这是在摄像头本地运行的高效算法。它在每个时间步内，动态维护一个由连续方向组成的“探索区域”，并基于近似模型的快速评估结果，智能地用更有潜力的新方向替换掉表现最差的旧方向。
*   **服务器-摄像头协同训练与推理 (Server-Camera Co-training and Inference)**：作为“学习和执行引擎”，服务器不仅负责对摄像头筛选出的最佳图像进行最终的高精度推理，还承担着训练、更新和下发近似模型的任务，形成了一个完整的“探索-验证-学习”闭环。

### 3. 效果：分析准确率和资源效率全面领先，开辟优化新维度

**准确率与资源效率增益**：
*   与最优的固定摄像头方案相比，在消耗同等资源的前提下，MadEye可将视频分析任务的中位数准确率提升 **2.9% - 25.7%**。
*   反之，要达到与MadEye相同的准确率，需要部署 **2-3.7倍** 数量的固定摄像头，这意味着MadEye能带来巨大的硬件和带宽成本节省。
*   其自身算法在边缘设备（Jetson Nano）上的开销极低，完全满足实时运行要求。

**与现有技术的互补性与优越性**：
*   在与多种摄像头自适应基线方法（Panoptes, MAB, Tracking）的对比中，MadEye的准确率取得了全面、压倒性的领先（例如，比MAB高**5.8倍**）。
*   其优越性根源在于其决策是基于**当前场景的实时内容**，而非依赖过时的历史数据或简单的规则。这使其能更快速、更精准地响应动态变化的场景。
*   MadEye可以和现有的其他视频流优化技术（如Chameleon）**完美互补**，在其他技术节省资源的基础上，进一步“免费”地提升系统准确率。

### NeuroScaler: Neural Video Enhancement at Scale
**本文由韩国科学技术院 (KAIST) 完成，发表在会议 ACM SIGCOMM '22，代码见：https://github.com/kaist-ina/neuroscaler-public。**

### 1. 动机：神经网络视频增强技术因成本过高而难以规模化

**问题核心：** 在直播服务器端通过AI技术提升视频质量前景广阔，但现有方案在实际部署中面临三大严峻挑战，导致其计算成本高昂，无法支持商业级规模（如Twitch的10万路并发流）。
*   **昂贵的端到端增强流程**：完整的“解码-AI推理-重编码”流程中，**AI超分辨率**和**高分辨率视频编码**这两个环节都极其消耗计算资源，共同构成了难以逾越的成本壁垒。
*   **低效的选择性增强策略**：为了降低成本，现有方法尝试只对部分“重要”帧进行增强。然而，如何“选择”这些帧本身就是一个难题。最先进的方案（如NEMO）需要运行昂贵的预推理来辅助决策，这并未从根本上解决问题，且不适用于实时直播。
*   **无效的资源调度**：当面临大规模并发流时，传统的基于流的负载均衡策略完全失效。由于不同视频内容的增强难度和收益各异，这种粗粒度的调度会导致严重的GPU资源浪费和负载不均，无法最大化集群的整体质量增益。

### 2. 方法：提出首个可扩展的、端到端协同优化的神经增强框架 (NeuroScaler)

该框架的核心思想是，通过一系列紧密耦合的系统设计和算法创新，对视频增强的全流程进行深度优化，从而在数量级上降低成本，实现真正的可扩展性。

**1. 核心组件 I：Anchor 调度器 (Anchor Scheduler)**
*   **零推理锚点帧选择 (Zero-Inference Anchor Frame Selection)**：作为框架的“决策大脑”，此模块是NeuroScaler的关键创新。它深刻洞察到，无需运行AI模型，仅利用视频**编码器自身产生的元数据**（如帧类型和残差信息），就足以高效、准确地预估增强每一帧的潜在收益。该算法完全在CPU上运行，速度极快，解决了选择性增强的决策瓶颈。
*   **锚点感知的资源管理 (Anchor-Aware Resource Management)**：作为框架的“指挥中心”，它采用**中心化的全局视角**，从所有直播流中统一挑选出“性价比”最高的增强任务（即锚点帧）。随后，它以“任务”为粒度，而非以“流”为粒度，将这些锚点帧增强任务精细地分配给集群中的各个GPU，从而实现了最优的负载均衡和资源利用。

**2. 核心组件 II：Anchor 增强器 (Anchor Enhancer)**
*   **混合视频编码 (Hybrid Video Encoding)**：作为框架的“效率核心”，这一设计巧妙地**绕过**了最昂贵的高清视频重编码环节。它并不生成一个全新的高清视频流，而是将**原始的低分辨率视频流**与**被单独编码为高质量图片的高清锚点帧**打包在一起。真正的视频合成在客户端完成，服务器端的编码开销因此被降至最低。
*   **GPU上下文切换优化 (GPU Context Switching Optimization)**：这是一个关键的底层工程优化。通过模型预编译和内存预分配等技术，NeuroScaler将多模型、多任务间的GPU切换开销从秒级降至毫秒甚至微秒级，确保了在高并发场景下GPU推理的高效性。

### 3. 效果：吞吐量与成本效益实现数量级突破，为大规模部署铺平道路

**端到端性能增益**：
*   与对每帧进行增强的基线方案相比，在达到同等画质的前提下，NeuroScaler将系统吞吐量提升了 **10倍**，而处理每路流的成本则惊人地降低了 **22.3倍**。
*   相较于传统的选择性增强方案，成本也降低了 **3.0 - 11.1倍**，吞吐量提升了 **2.5 - 5倍**。
*   案例研究表明，在一个Twitch规模（10万路并发流）的场景中部署NeuroScaler，每小时的运营成本约为**7,898美元**，完全在商业可接受范围内。

**各组件的独立贡献**：
*   实验通过“逐一添加优化”的方式清晰地证明了NeuroScaler每个组件的价值：**GPU上下文优化**是系统能够实时运行的基础；**混合编码**将吞吐量提升了2.16倍；而**零推理锚点选择**则在其基础上将吞吐量再次提升了2.30倍。
*   这证明了NeuroScaler的成功并非来自单一的奇技淫巧，而是一整套协同工作的系统性解决方案的胜利。


### NEMO: Enabling Neural-enhanced Video Streaming on Commodity Mobile Devices
**本文由韩国科学技术院 (KAIST) 完成，发表在会议 ACM MobiCom '20，代码见：https://github.com/kaist-ina/nemo。**

### 1. 动机：AI视频增强在移动设备上面临严峻的性能与功耗瓶颈

**问题核心：** 将AI超分辨率技术移植到移动端以提升视频体验，是技术发展的必然趋势。然而，这一美好的愿景被移动设备固有的三大资源限制所阻碍，使得传统的“每帧计算”方案完全不可行。
*   **计算性能瓶颈**：移动设备GPU的算力远逊于桌面级显卡。实测表明，在高端手机上运行超分辨率模型，处理速度仅为5-10fps，远低于实时播放所需的流畅度，导致视频严重卡顿。
*   **能耗与电池寿命危机**：AI推理是耗电大户，其功耗比普通视频播放高出5.9至18.1倍。这意味着手机的续航时间会从十几个小时锐减到不足两小时，对移动用户来说是不可接受的。
*   **设备过热问题**：持续的高强度计算导致手机迅速升温，表面温度可轻易超过40°C。这不仅会引起用户不适，还可能触发系统降频保护，进一步恶化性能，甚至存在低温烫伤的风险。

### 2. 方法：提出基于“选择性计算与结果复用”的服务器-客户端协同框架 (NEMO)

NEMO的核心思想是，不再对每一帧都进行昂贵的AI计算，而是通过服务器与客户端的智能协同，只对视频中一小部分精心挑选的帧（**锚点, Anchor Points**）在移动端执行AI增强，然后高效地复用这些高质量结果来“重建”其余所有帧。

**1. 设计核心 I：如何选择锚点 (Anchor Point Selection) 与保证质量**
*   **服务器端离线分析**：NEMO在视频分发前，于服务器端进行一次性的离线分析。它通过一个高效的贪心算法，迭代地选择出能最大化质量收益的“锚点”帧。
*   **基于质量损失的优化目标**：该算法的目标是，在保证增强后的视频质量与“理想状态”（即每帧都进行增强）相比，其**质量损失（缓存侵蚀, Cache Erosion）**被严格控制在一个预设的阈值内（如PSNR下降<0.5dB）的前提下，使用的锚点数量最少。
*   **生成缓存配置文件 (Cache Profile)**：最终选定的锚点信息被编码成一个极小的“缓存配置文件”，随视频一同下发。这个文件就是客户端执行智能决策的“说明书”。

**2. 设计核心 II：如何复用结果 (SR-Integrated Codec)**
*   **深度集成的解码器**：NEMO并未将AI模块作为播放器的上层应用，而是将超分辨率功能**深度集成到底层视频解码器（libvpx）内部**。这种设计让它能访问到最精细的帧间依赖信息。
*   **利用编码信息进行重建**：当解码一个非锚点帧时，该集成解码器会：
    1.  从缓存中查找最近的高质量锚点帧作为参考。
    2.  利用当前帧在原始视频流中的**运动矢量 (Motion Vector)** 和 **残差 (Residual)** 信息。
    3.  通过轻量级的运动补偿和残差叠加操作，快速、精确地“重建”出当前帧的高质量版本。
*   **高效的客户端执行流程**：锚点帧的处理（AI推理）在GPU上进行，而非锚点帧的重建则在CPU上高效完成。整个过程被封装在解码器API内部，对上层应用透明。

### 3. 效果：首次在商用移动设备上实现可用的、节能的实时AI视频增强

**性能、能耗与温度的全面优化**：
*   **实时性能达成**：与“每帧增强”方案相比，NEMO将视频处理吞吐量平均提升了 **11.5倍**，在多种商用手机上均实现了远超30fps的实时处理能力。
*   **能耗与发热显著降低**：NEMO将AI增强带来的额外能耗降低了 **88.6%**，并将设备表面温度成功控制在**35°C以下**的用户舒适区内。
*   **极高的计算节省**：在保证高质量的前提下（平均PSNR损失仅0.41dB），NEMO仅对视频中 **1.8%至9.7%** 的帧进行了昂贵的AI计算，其余超过90%的帧都通过高效复用解决。

**对用户体验质量 (QoE) 的实际提升**：
*   在真实的移动网络环境下，相比仅使用传统自适应码率（ABR）技术的播放器，NEMO通过利用终端算力，能够将用户的综合体验质量（QoE）平均提升 **31.2%**。
*   这一提升是“免费”的，因为它不消耗任何额外的网络带宽。反之，它也可以在提供相同体验质量的前提下，为用户节省 **18.3% - 22.1%** 的数据流量。
